{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydlAa2C-rSeh"
      },
      "source": [
        "# Lesson 6: Improve Agent's GPA\n",
        "\n",
        "In this lesson, you'll make two targeted changes to the agent:\n",
        "\n",
        "1. Adjust the planning prompt to include explicit goals, pre-conditions, and post-conditions for each step. This helps the executor understand the sub-goals it needs to reach.\n",
        "\n",
        "2. You will add inline evals so the agent receives feedback on when to do additional research. This provides the executor feedback on whether it's reaching its sub-goals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtqgaXFU4_iv"
      },
      "source": [
        "```+---------------------------------------------------------------------------------------------------------+\n",
        "|                                    ARCHITECTURE DU SYSTEME MULTI-AGENTS                                 |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                          [User Query]\n",
        "                                                |\n",
        "                                                v\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|  1. PLANNER NODE (Le Cerveau)                                                                           |\n",
        "|  -----------------------------------------------------------------------------------------------------  |\n",
        "|  * Fonction : `planner_node(state)`                                                                     |\n",
        "|  * Prompt   : `patched_plan_prompt` (qui utilise RECURSION_LIMIT pour g√©rer le budget)                  |\n",
        "|  * Sortie   : G√©n√®re un plan JSON (Step 1, Step 2, ...) ajout√© au `state['plan']`                       |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "          +---------------------------------------------------------------------------+\n",
        "          |                                                                           |\n",
        "          |   2. EXECUTOR NODE (Le Chef d'Orchestre)    <-------------------------+   |\n",
        "          |   -----------------------------------------------------------------   |   |\n",
        "          |   * Fonction : `executor_node(state)`                                 |   |\n",
        "          |   * Logique  : Lit le `state['plan']`, v√©rifie `current_step`.        |   |\n",
        "          |                D√©cide quel agent appeler (Routing).                   |   |\n",
        "          |                G√®re les drapeaux `replan` et `previous_step_failed`.  |   |\n",
        "          |                                                                           |\n",
        "          +----------------------+----------------------+-------------------------+---+\n",
        "                                 |                      |                         |\n",
        "        +------------------------+                      |                         |\n",
        "        | (Route: \"cortex_researcher\")                  | (Route: \"web_...\")      | (Route: \"chart_...\")\n",
        "        v                                               v                         v\n",
        "+------------------------------------+    +-----------------------------+    +-----------------------------+\n",
        "| 3a. CORTEX RESEARCH NODE           |    | 3b. WEB RESEARCH NODE       |    | 3c. CHART GENERATOR NODE    |\n",
        "| ---------------------------------- |    | --------------------------- |    | --------------------------- |\n",
        "| * Func: `cortex_agents_research_...|    | * Func: `web_research_node` |    | * Func: `chart_node`        |\n",
        "| * Outils :                         |    | * Agent : ReAct Agent       |    | * Agent : ReAct Agent       |\n",
        "|   - `wikipedia_rag_tool`           |    | * Outil : `tavily_tool`     |    | * Outil : `PythonREPL`      |\n",
        "|   - `wikidata_sparql_tool`         |    |                             |    |                             |\n",
        "| * Note: Utilise `_cortex_llm_...`  |    |                             |    |                             |\n",
        "+-----------------+------------------+    +--------------+--------------+    +--------------+--------------+\n",
        "                  |                                      |                                  |\n",
        "                  |                                      |                                  |\n",
        "                  +-------------------+------------------+----------------------------------+\n",
        "                                      |\n",
        "                                      | (R√©sultat de l'√©tape / Demande de Replanification)\n",
        "                                      | Retour vers Executor\n",
        "                                      |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|                                     CONDITION DE FIN                                                    |\n",
        "| Si (toutes les √©tapes finies) OU (replanification impossible) OU (remaining_steps <= 0)                 |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "|  4. SYNTHESIZER NODE (Le R√©dacteur)                                                                     |\n",
        "|  -----------------------------------------------------------------------------------------------------  |\n",
        "|  * Fonction : `synthesizer_node(state)`                                                                 |\n",
        "|  * Prompt   : `final_answer_prompt`                                                                     |\n",
        "|  * Entr√©e   : Prend tout l'historique des messages et des r√©sultats d'outils.                           |\n",
        "|  * Sortie   : R√©ponse finale structur√©e pour l'utilisateur.                                             |\n",
        "+---------------------------------------------------------------------------------------------------------+\n",
        "                                                |\n",
        "                                                v\n",
        "                                          [Final Answer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZU50-durbyM",
        "outputId": "70ccf6ce-d226-4d05-e419-ce8e3cde8ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Setup (will kill 1st time then, re-launch)\n",
        "import os, sys, time\n",
        "if os.path.exists(\".lib_installed\"):\n",
        "    print(\"Dependencies installed.\")\n",
        "else:\n",
        "  !pip install -q \\\n",
        "      langchain \\\n",
        "      langchain-core \\\n",
        "      langchain-community \\\n",
        "      langchain-openai \\\n",
        "      langchain-experimental \\\n",
        "      langchain-tavily \\\n",
        "      langgraph \\\n",
        "      trulens-core trulens-providers-openai trulens-apps-langgraph trulens-dashboard \\\n",
        "      opentelemetry-sdk nest-asyncio2 openinference-instrumentation-langchain arize-phoenix uvicorn \\\n",
        "      python-dotenv \\\n",
        "      wikipedia \\\n",
        "      SPARQLWrapper\n",
        "\n",
        "  with open(\".lib_installed\", \"w\") as f: f.write(\"Installation OK\")\n",
        "\n",
        "  # # Si on est dans Colab, on tue le processus pour forcer le rechargement des nouvelles librairies\n",
        "  # if \"google.colab\" in sys.modules:\n",
        "  #     print(\"üîÑ Red√©marrage automatique de la session pour appliquer les mises √† jour... ‚ö†Ô∏è (Vous verrez peut-√™tre une notification 'Session √©cras√©e', c'est normal !)\")\n",
        "  #     time.sleep(1)\n",
        "  #     os.kill(os.getpid(), 9)\n",
        "\n",
        "import nest_asyncio2 as nest_asyncio\n",
        "nest_asyncio.apply()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qenKYEtHrq0V"
      },
      "outputs": [],
      "source": [
        "# @title 2. Central Configuration & Secrets\n",
        "import os\n",
        "groq = True\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"XXX\"  # PASTE TAVILY KEY\n",
        "\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.groq.com/openai/v1\" # OPENROUTER: \"https://openrouter.ai/api/v1\", GROQ: \"https://api.groq.com/openai/v1\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"XXX\" # OPENROUTER: \"sk-XXX\", GROQ:\n",
        "# --- 2. MODELS DEFINITION\n",
        "if \"groq\" in os.environ[\"OPENAI_BASE_URL\"]:\n",
        "    os.environ[\"MODEL_EXECUTOR\"] = \"llama-3.1-8b-instant\" # \"llama-3.3-70b-versatile\" # Llama 3.3 70B est le plus polyvalent (\"Versatile\") pour la r√©daction et la synth√®se.\n",
        "    os.environ[\"MODEL_REASONING\"] = \"llama-3.1-8b-instant\" # \"llama-3.3-70b-versatile\" # On utilise DeepSeek R1 (version distill√©e sur Llama 70B) - C'est ACTUELLEMENT le meilleur mod√®le de raisonnement gratuit sur Groq (\"Thinking Model\").\n",
        "    os.environ[\"MODEL_EVAL\"] = \"llama-3.1-8b-instant\" # \"llama-3.3-70b-versatile\" # On r√©utilise Llama 3.3 pour avoir une critique de qualit√©. - Si vous avez trop d'erreurs 429 (quota), remplacez celui-ci par \"llama-3.1-8b-instant\"\n",
        "else:\n",
        "    os.environ[\"MODEL_EXECUTOR\"] = \"openai/gpt-5-nano\" # \"google/gemini-2.0-flash-lite-001\" # \"openai/gpt-5-nano\"\n",
        "    os.environ[\"MODEL_REASONING\"] = \"openai/gpt-5-nano\" # \"openai/gpt-oss-120b\" # or \"openai/o3-mini\"\n",
        "    os.environ[\"MODEL_EVAL\"] = \"openai/gpt-5-nano\" # \"google/gemini-2.0-flash-lite-001\" # \"deepseek/deepseek-r1-distill-qwen-14b\" # \"openai/gpt-5-nano\"\n",
        "\n",
        "# --- 3. TRULENS SETUP ---\n",
        "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ttQS-dwMOcj"
      },
      "source": [
        "# Setup a rate limiter to ensure to enjoy free Groq API :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1blwT4ZL-mq",
        "outputId": "edcb8739-93a5-4ee4-9f13-d5f72d39ce12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ°Ô∏è Groq Armor activ√© : 20 RPM + Auto-Retry sur 429 (LangChain & TruLens prot√©g√©s)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import openai\n",
        "from collections import deque\n",
        "\n",
        "# --- CONFIGURATION (Safe Mode) ---\n",
        "RPM_LIMIT = 20        # On vise 20 pour rester sous les 30 (marge de s√©cu)\n",
        "MAX_RETRIES = 5       # Nombre d'essais en cas d'erreur 429\n",
        "BASE_SLEEP = 2        # Temps d'attente initial (backoff exponentiel)\n",
        "\n",
        "# Stockage des timestamps pour le Rate Limiter\n",
        "_timestamps = deque(maxlen=RPM_LIMIT)\n",
        "\n",
        "# On patch le niveau le plus bas : la m√©thode `request` du client HTTP interne\n",
        "# Cela couvre TOUT : LangChain, TruLens, appels directs, nouveaux imports.\n",
        "if not hasattr(openai._base_client.SyncHttpxClientWrapper, \"_original_request\"):\n",
        "    openai._base_client.SyncHttpxClientWrapper._original_request = openai._base_client.SyncHttpxClientWrapper.request\n",
        "\n",
        "def protected_request(self, *args, **kwargs):\n",
        "    # 1. Rate Limiter Pr√©ventif (Sliding Window)\n",
        "    if len(_timestamps) == RPM_LIMIT:\n",
        "        elapsed = time.time() - _timestamps[0]\n",
        "        if elapsed < 60:\n",
        "            time.sleep(60 - elapsed + 0.5)\n",
        "\n",
        "    # 2. Retry Logic pour erreur 429 (Tokens/TPM)\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            # Appel r√©el\n",
        "            response = self._original_request(*args, **kwargs)\n",
        "            _timestamps.append(time.time()) # Succ√®s -> on note l'heure\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            # On d√©tecte l'erreur 429 (Too Many Requests)\n",
        "            if \"429\" in str(e) and attempt < MAX_RETRIES - 1:\n",
        "                wait = BASE_SLEEP * (2 ** attempt) # 2s, 4s, 8s, 16s...\n",
        "                print(f\"‚ö†Ô∏è Quota Groq atteint (429). Pause de {wait}s...\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                raise e # Autre erreur ou max retries -> on plante\n",
        "\n",
        "# Application du patch\n",
        "openai._base_client.SyncHttpxClientWrapper.request = protected_request\n",
        "print(f\"üõ°Ô∏è Groq Armor activ√© : {RPM_LIMIT} RPM + Auto-Retry sur 429 (LangChain & TruLens prot√©g√©s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UGjO3Nh3OTWH",
        "outputId": "e1835730-44e8-4f22-ab1b-b1a28e6db553"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNI [phoenix.session.session] Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
            "ERROR [opentelemetry.sdk._shared_internal] Exception while exporting Span.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 494, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1037, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 325, in connect\n",
            "    self.sock = self._new_conn()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    if isinstance(e.reason, ResponseError):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 157, in _export\n",
            "    resp = self._session.post(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 700, in send\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9aa13380>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 494, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1333, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1093, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1037, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 325, in connect\n",
            "    self.sock = self._new_conn()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    if isinstance(e.reason, ResponseError):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/sdk/_shared_internal/__init__.py\", line 179, in _export\n",
            "    self._exporter.export(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 182, in export\n",
            "    resp = self._export(serialized_data, deadline_sec - time())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py\", line 165, in _export\n",
            "    resp = self._session.post(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 700, in send\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=6002): Max retries exceeded with url: /v1/traces (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8e9a962de0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "WARNI [phoenix.session.session] ‚ö†Ô∏è PHOENIX_COLLECTOR_ENDPOINT is set to http://localhost:6002/v1/traces.\n",
            "‚ö†Ô∏è This means that traces will be sent to the collector endpoint and not this app.\n",
            "‚ö†Ô∏è If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
            "‚ö†Ô∏è You will need to restart your notebook to apply this change.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç To view the Phoenix app in your browser, visit https://nelnk6aryaa9-496ff2e9c6d22116-6002-colab.googleusercontent.com/\n",
            "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
            "üöÄ Phoenix UI is ready at: https://nelnk6aryaa9-496ff2e9c6d22116-6002-colab.googleusercontent.com/\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n    if (!google.colab.kernel.accessAllowed && !cache) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port, {cache});\n    const iframe = document.createElement('iframe');\n    iframe.src = new URL(path, url).toString();\n    iframe.height = height;\n    iframe.width = width;\n    iframe.style.border = 0;\n    iframe.allow = [\n        'accelerometer',\n        'autoplay',\n        'camera',\n        'clipboard-read',\n        'clipboard-write',\n        'gyroscope',\n        'magnetometer',\n        'microphone',\n        'serial',\n        'usb',\n        'xr-spatial-tracking',\n    ].join('; ');\n    element.appendChild(iframe);\n  })(6002, \"/\", \"100%\", 1000, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Initiate üöÄ Phoenix monitoring of Langchain / LangGraph\n",
        "import phoenix as px\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "import os, time\n",
        "from google.colab import output\n",
        "\n",
        "os.environ[\"PHOENIX_PORT\"] = \"6002\" # Petite s√©curit√© pour √©viter le conflit de ports si vous relancez plusieurs fois\n",
        "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:\" + os.environ[\"PHOENIX_PORT\"] + \"/v1/traces\"\n",
        "os.environ[\"PHOENIX_PROJECT_NAME\"] = \"langgraph-data-toulon\"\n",
        "\n",
        "try:\n",
        "    phoenix_session = px.launch_app() # 1. Lancer l'UI locale\n",
        "    time.sleep(5) # Give it time to spin up\n",
        "    print(f\"üöÄ Phoenix UI is ready at: {phoenix_session.url}\")\n",
        "    # try: # L'instumentation est d√©plac√©e apr√®s l'initialisation de TruGraph pour se brancher dessus pour permettre de partager le flux OTEL\n",
        "    #     LangChainInstrumentor().instrument() # 2. Activer l'instrumentation\n",
        "    #     print(\"‚úÖ Instrumentation activ√©e.\") # LangChainInstrumentor capture aussi les noeuds LangGraph de base\n",
        "    # except Exception as e:\n",
        "    #     print(f\"‚ö†Ô∏è Erreur d'instrumentation (peut-√™tre d√©j√† active): {e}\")\n",
        "    output.serve_kernel_port_as_iframe(os.environ[\"PHOENIX_PORT\"], height=1000) # Cela ouvre une fen√™tre directement dans le notebook\n",
        "except Exception as e:\n",
        "    print(f\"Erreur au lancement: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV6A4xnNrweW"
      },
      "source": [
        "Cr√©ation de prompts.py, helper.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IylUwZSkrsRf"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#%%writefile prompts.py\n",
        "from typing import Dict, Any, List\n",
        "#from langchain.schema import HumanMessage  # type: ignore[import-not-found]\n",
        "from langchain_core.messages import HumanMessage\n",
        "import json\n",
        "from typing import Optional\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.types import Command\n",
        "from typing import Literal, Optional, List, Dict, Any, Type\n",
        "\n",
        "MAX_REPLANS = 2\n",
        "\n",
        "# Custom State class with specific keys\n",
        "class State(MessagesState):\n",
        "    enabled_agents: Optional[List[str]]\n",
        "    # Current plan only: mapping from step number (as string) to step definition\n",
        "    plan: Optional[Dict[str, Dict[str, Any]]]\n",
        "    user_query: Optional[str]\n",
        "    current_step: int\n",
        "    replan_flag: Optional[bool]\n",
        "    last_reason: Optional[str]\n",
        "    # Replan attempts tracked per step number\n",
        "    replan_attempts: Optional[Dict[int, int]]\n",
        "    agent_query: Optional[str]\n",
        "\n",
        "def get_agent_descriptions() -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Return structured agent descriptions with capabilities and guidelines.\n",
        "    Edit this function to change how the planner/executor reason about agents.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"web_researcher\": {\n",
        "            \"name\": \"Web Researcher\",\n",
        "            \"capability\": \"Fetch public data via Tavily web search\",\n",
        "            \"use_when\": \"Public information, news, current events, or external facts are needed\",\n",
        "            \"limitations\": \"Cannot access private/internal company data\",\n",
        "            \"output_format\": \"Raw research data and findings from public sources\",\n",
        "        },\n",
        "        \"cortex_researcher\": {\n",
        "            \"name\": \"Cortex Researcher\",\n",
        "            # \"capability\": \"Query private/company data in Snowflake, including structured deal records (company name, deal value, sales rep, close date, deal status, product line) and unstructured sales meeting notes, via Snowflake Cortex Agents.\",\n",
        "            # \"use_when\": \"Internal documents, company databases, or private data access is required\",\n",
        "            # \"limitations\": \"Cannot access public web data\",\n",
        "            # \"output_format\": \"For structured requests, return the exact fields and include SQL when applicable; for unstructured, return concise relevant excerpts with citations.\",\n",
        "            \"capability\": \"Query general knowledge. Use Wikidata for structured facts (dates, lists, counts) and Wikipedia for unstructured summaries.\",\n",
        "            \"use_when\": \"Questions about real-world entities, history, or factual lists.\",\n",
        "            \"limitations\": \"Cannot access private company data.\",\n",
        "            \"output_format\": \"Structured tables (Wikidata) or text summaries (Wikipedia).\",\n",
        "            },\n",
        "        \"chart_generator\": {\n",
        "            \"name\": \"Chart Generator\",\n",
        "            \"capability\": \"Build visualizations from structured data\",\n",
        "            \"use_when\": \"User explicitly requests charts, graphs, plots, visualizations (keywords: chart, graph, plot, visualise, bar-chart, line-chart, histogram, etc.)\",\n",
        "            \"limitations\": \"Requires structured data input from previous steps\",\n",
        "            \"output_format\": \"Visual charts and graphs\",\n",
        "            \"position_requirement\": \"Must be used as final step after data gathering is complete\",\n",
        "        },\n",
        "        \"chart_summarizer\": {\n",
        "            \"name\": \"Chart Summarizer\",\n",
        "            \"capability\": \"Summarize and explain chart visualizations\",\n",
        "            \"use_when\": \"After chart_generator has created a visualization\",\n",
        "            \"limitations\": \"Requires a chart as input\",\n",
        "            \"output_format\": \"Written summary and analysis of chart content\",\n",
        "        },\n",
        "        \"synthesizer\": {\n",
        "            \"name\": \"Synthesizer\",\n",
        "            \"capability\": \"Write comprehensive prose summaries of findings\",\n",
        "            \"use_when\": \"Final step when no visualization is requested - combines all previous research\",\n",
        "            \"limitations\": \"Requires research data from previous steps\",\n",
        "            \"output_format\": \"Coherent written summary incorporating all findings\",\n",
        "            \"position_requirement\": \"Should be used as final step when no chart is needed\",\n",
        "        },\n",
        "    }\n",
        "\n",
        "def _get_enabled_agents(state: State | None = None) -> List[str]:\n",
        "    \"\"\"Return enabled agents; if absent, use baseline/default.\n",
        "\n",
        "    Supports both dict-style and attribute-style state objects.\n",
        "    \"\"\"\n",
        "    baseline = [\"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"]\n",
        "    if not state:\n",
        "        return baseline\n",
        "    val = state.get(\"enabled_agents\") if hasattr(state, \"get\") else getattr(state, \"enabled_agents\", None)\n",
        "\n",
        "    if isinstance(val, list) and val:\n",
        "        allowed = {\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"}\n",
        "        filtered = [a for a in val if a in allowed]\n",
        "        return filtered\n",
        "    return baseline\n",
        "\n",
        "def format_agent_list_for_planning(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent descriptions for the planning prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled_list = _get_enabled_agents(state)\n",
        "    agent_list = []\n",
        "\n",
        "    for agent_key, details in descriptions.items():\n",
        "        if agent_key not in enabled_list:\n",
        "            continue\n",
        "        agent_list.append(f\"  ‚Ä¢ `{agent_key}` ‚Äì {details['capability']}\")\n",
        "\n",
        "    return \"\\n\".join(agent_list)\n",
        "\n",
        "def format_agent_guidelines_for_planning(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent usage guidelines for the planning prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled = set(_get_enabled_agents(state))\n",
        "    guidelines = []\n",
        "\n",
        "    # Cortex vs Web researcher (only include guidance for enabled agents)\n",
        "    if \"cortex_researcher\" in enabled:\n",
        "        guidelines.append(f\"- Use `cortex_researcher` when {descriptions['cortex_researcher']['use_when'].lower()}.\")\n",
        "    if \"web_researcher\" in enabled:\n",
        "        guidelines.append(f\"- Use `web_researcher` for {descriptions['web_researcher']['use_when'].lower()}.\")\n",
        "\n",
        "    # Chart generator specific rules\n",
        "    if \"chart_generator\" in enabled:\n",
        "        chart_desc = descriptions['chart_generator']\n",
        "        cs_hint = \" A `chart_summarizer` should be used to summarize the chart.\" if \"chart_summarizer\" in enabled else \"\"\n",
        "        guidelines.append(f\"- **Include `chart_generator` _only_ if {chart_desc['use_when'].lower()}**. Do NOT use it for text summaries, news articles, or lists of topics. If included, `chart_generator` must be {chart_desc['position_requirement'].lower()}.\")\n",
        "\n",
        "    # Synthesizer default\n",
        "    if \"synthesizer\" in enabled:\n",
        "        synth_desc = descriptions['synthesizer']\n",
        "        guidelines.append(f\"  ‚Äì Otherwise use `synthesizer` as {synth_desc['position_requirement'].lower()}, and be sure to include all of the data from the previous steps.\")\n",
        "\n",
        "    return \"\\n\".join(guidelines)\n",
        "\n",
        "def format_agent_guidelines_for_executor(state: State | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Format agent usage guidelines for the executor prompt.\n",
        "    \"\"\"\n",
        "    descriptions = get_agent_descriptions()\n",
        "    enabled = _get_enabled_agents(state)\n",
        "    guidelines = []\n",
        "\n",
        "    if \"web_researcher\" in enabled:\n",
        "        web_desc = descriptions['web_researcher']\n",
        "        guidelines.append(f\"- Use `\\\"web_researcher\\\"` when {web_desc['use_when'].lower()}.\")\n",
        "    if \"cortex_researcher\" in enabled:\n",
        "        cortex_desc = descriptions['cortex_researcher']\n",
        "        guidelines.append(f\"- Use `\\\"cortex_researcher\\\"` for {cortex_desc['use_when'].lower()}.\")\n",
        "\n",
        "    return \"\\n\".join(guidelines)\n",
        "\n",
        "def plan_prompt(state: State) -> HumanMessage:\n",
        "    \"\"\"\n",
        "    Build the prompt that instructs the LLM to return a high‚Äëlevel plan.\n",
        "    \"\"\"\n",
        "    replan_flag   = state.get(\"replan_flag\", False)\n",
        "    user_query    = state.get(\"user_query\", state[\"messages\"][0].content)\n",
        "    prior_plan    = state.get(\"plan\") or {}\n",
        "    replan_reason = state.get(\"last_reason\", \"\")\n",
        "\n",
        "    # Get agent descriptions dynamically\n",
        "\n",
        "    agent_list = format_agent_list_for_planning(state)\n",
        "    agent_guidelines = format_agent_guidelines_for_planning(state)\n",
        "\n",
        "    enabled_list = _get_enabled_agents(state)\n",
        "\n",
        "    # Build planner agent enum based on enabled agents\n",
        "    enabled_for_planner = [\n",
        "        a for a in enabled_list\n",
        "        if a in (\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"synthesizer\")\n",
        "    ]\n",
        "    planner_agent_enum = \" | \".join(enabled_for_planner) or \"web_researcher | chart_generator | synthesizer\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "        You are the **Planner** in a multi‚Äëagent system.  Break the user's request\n",
        "        into a sequence of numbered steps (1,‚ÄØ2,‚ÄØ3, ‚Ä¶).  **There is no hard limit on\n",
        "        step count** as long as the plan is concise and each step has a clear goal.\n",
        "\n",
        "        You may decompose the user's query into sub-queries, but **prioritize grouping related information retrieval**.\n",
        "        Avoid creating unnecessary granular steps in order to save execution budget while maintaining quality.\n",
        "\n",
        "        For example, \"Find the top 5 cities AND their populations\" should be a SINGLE step, not two.\n",
        "\n",
        "        However, if the user's query is \"What were the key\n",
        "        action items in the last quarter, and what was a recent news story for\n",
        "        each of them?\", you may break it into steps:\n",
        "\n",
        "        1. Fetch the key action items in the last quarter.\n",
        "        2. Fetch a recent news story for the first action item.\n",
        "        3. Fetch a recent news story for the second action item.\n",
        "        4. Fetch a recent news story for the last action item\n",
        "\n",
        "        Here is a list of available agents you can call upon to execute the tasks in your plan. You may call only one agent per step.\n",
        "\n",
        "        {agent_list}\n",
        "\n",
        "        Return **ONLY** valid JSON (no markdown, no explanations) in this form:\n",
        "\n",
        "        {{\n",
        "        \"1\": {{\n",
        "            \"agent\": \"{planner_agent_enum}\",\n",
        "            \"action\": \"string\",\n",
        "        }},\n",
        "        \"2\": {{ ... }},\n",
        "        \"3\": {{ ... }}\n",
        "        }}\n",
        "\n",
        "        Guidelines:\n",
        "        {agent_guidelines}\n",
        "        \"\"\"\n",
        "\n",
        "    if replan_flag:\n",
        "        prompt += f\"\"\"\n",
        "        The current plan needs revision because: {replan_reason}\n",
        "\n",
        "        Current plan:\n",
        "        {json.dumps(prior_plan, indent=2)}\n",
        "\n",
        "        When replanning:\n",
        "        - Focus on UNBLOCKING the workflow rather than perfecting it.\n",
        "        - Only modify steps that are truly preventing progress.\n",
        "        - Prefer simpler, more achievable alternatives over complex rewrites.\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        prompt += \"\\nGenerate a new plan from scratch.\"\n",
        "\n",
        "    prompt += f'\\nUser query: \"{user_query}\"'\n",
        "\n",
        "    return HumanMessage(content=prompt)\n",
        "\n",
        "#@instrument(attributes=lambda ret, exception, *args, **kwargs: {\"retrieved_execution\": ret.update.get(\"messages\", [HumanMessage(content=\"\")])[-1].content})\n",
        "def executor_prompt(state: State) -> HumanMessage:\n",
        "    \"\"\"\n",
        "    Build the single‚Äëturn JSON prompt that drives the executor LLM.\n",
        "    \"\"\"\n",
        "    step = int(state.get(\"current_step\", 0))\n",
        "    latest_plan: Dict[str, Any] = state.get(\"plan\") or {}\n",
        "    plan_block: Dict[str, Any] = latest_plan.get(str(step), {})\n",
        "    max_replans    = MAX_REPLANS\n",
        "    attempts       = (state.get(\"replan_attempts\", {}) or {}).get(step, 0)\n",
        "\n",
        "    # Get agent guidelines dynamically\n",
        "    executor_guidelines = format_agent_guidelines_for_executor(state)\n",
        "    plan_agent = plan_block.get(\"agent\", \"web_researcher\")\n",
        "\n",
        "    messages_tail = (state.get(\"messages\") or [])[-4:]\n",
        "\n",
        "    executor_prompt = f\"\"\"\n",
        "        **IMPORTANT:** Respond **ONLY** with a valid JSON object. Do NOT include any additional text, explanation, or conversational phrases, such as \"FINAL ANSWER\".\n",
        "\n",
        "        {{\n",
        "        \"replan\": <true|false>,\n",
        "        \"goto\": \"<{ '|'.join([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']) }>\",\n",
        "        \"reason\": \"<1 sentence>\",\n",
        "        \"query\": \"<text>\"\n",
        "        }}\n",
        "\n",
        "        You are the **executor** in a multi‚Äëagent system with these agents:\n",
        "        `{ '`, `'.join(sorted(set([a for a in _get_enabled_agents(state) if a in ['web_researcher','cortex_researcher','chart_generator','chart_summarizer','synthesizer']] + ['planner']))) }`.\n",
        "\n",
        "        **Tasks**\n",
        "        1. Decide if the current plan needs revision.  ‚Üí `\"replan_flag\": true|false`\n",
        "        2. Decide which agent to run next.             ‚Üí `\"goto\": \"<agent_name>\"`\n",
        "        3. Give one‚Äësentence justification.            ‚Üí `\"reason\": \"<text>\"`\n",
        "        4. Write the exact question that the chosen agent should answer\n",
        "                                                    ‚Üí \"query\": \"<text>\"\n",
        "\n",
        "        **Guidelines**\n",
        "        {executor_guidelines}\n",
        "        - After **{MAX_REPLANS}** failed replans for the same step, move on.\n",
        "        - If you *just replanned* (replan_flag is true) let the assigned agent try before\n",
        "        requesting another replan.\n",
        "\n",
        "        **PRIORITIZE FORWARD PROGRESS:** Only replan if the current step is completely blocked.\n",
        "        1. If any reasonable data was obtained that addresses the step's core goal, set `\"replan\": false` and proceed.\n",
        "        2. Set `\"replan\": true` **only if** ALL of these conditions are met:\n",
        "        ‚Ä¢ The step has produced zero useful information\n",
        "        ‚Ä¢ The missing information cannot be approximated or obtained by remaining steps\n",
        "        ‚Ä¢ `attempts < {max_replans}`\n",
        "        3. When `attempts == {max_replans}`, always move forward (`\"replan\": false`).\n",
        "\n",
        "        ### Decide `\"goto\"`\n",
        "        - If `\"replan\": true` ‚Üí `\"goto\": \"planner\"`.\n",
        "        - If current step has made reasonable progress ‚Üí move to next step's agent.\n",
        "        - Otherwise execute the current step's assigned agent (`{plan_agent}`).\n",
        "\n",
        "        ### Build `\"query\"`\n",
        "        Write a clear, standalone instruction for the chosen agent. If the chosen agent\n",
        "        is `web_researcher` or `cortex_researcher`, the query should be a standalone question,\n",
        "        written in plain english, and answerable by the agent.\n",
        "\n",
        "        Ensure that the query uses consistent language as the user's query.\n",
        "\n",
        "        Context you can rely on\n",
        "        - User query ..............: {state.get(\"user_query\")}\n",
        "        - Current step index ......: {step}\n",
        "        - Current plan step .......: {plan_block}\n",
        "        - Just‚Äëreplanned flag .....: {state.get(\"replan_flag\")}\n",
        "        - Previous messages .......: {messages_tail}\n",
        "        \"\"\"\n",
        "\n",
        "    return HumanMessage(\n",
        "        content=executor_prompt\n",
        "    )\n",
        "\n",
        "def agent_system_prompt(suffix: str) -> str:\n",
        "    return (\n",
        "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
        "        \" Use the provided tools to progress towards answering the question.\"\n",
        "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
        "        \" will help where you left off. Execute what you can to make progress.\"\n",
        "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
        "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
        "        f\"\\n{suffix}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH_8GCeQBOWb"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#%%writefile helper.py\n",
        "from __future__ import annotations\n",
        "# pyright: reportMissingImports=false, reportMissingTypeStubs=false, reportIncompatibleMethodOverride=false\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=r\"Valid config keys have changed in V2\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=r\"WARNING! response_format is not default parameter\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=r\"pkg_resources is deprecated as an API.*\", category=UserWarning, module=r\"^munch$\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import wikipedia # Addition\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from dotenv import load_dotenv\n",
        "#from snowflake.snowpark import Session\n",
        "from langchain_core.tools import tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from typing import Annotated, Literal, Optional, List, Dict, Any, Type\n",
        "from trulens.otel.semconv.trace import SpanAttributes\n",
        "from trulens.core.otel.instrument import instrument\n",
        "#from snowflake.core import Root\n",
        "#from snowflake.core.cortex.lite_agent_service import AgentRunRequest\n",
        "from pydantic import BaseModel, PrivateAttr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_tavily import TavilySearch\n",
        "#from langchain.schema import HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import MessagesState, START, StateGraph, END\n",
        "from langgraph.types import Command\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from trulens.core import Feedback, Select\n",
        "from trulens.core.feedback.selector import Selector\n",
        "#from trulens.core.feedback.selector import Selector\n",
        "from trulens.providers.openai import OpenAI\n",
        "import numpy as np\n",
        "#from prompts import plan_prompt, executor_prompt, agent_system_prompt\n",
        "\n",
        "from langgraph.managed.is_last_step import RemainingSteps\n",
        "\n",
        "# load full dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# --- HELPERS POUR SELECTION JSON (MODE SANS OTEL) ---\n",
        "def select_context(output):\n",
        "    return [m.content for m in output.get('messages', []) if getattr(m, 'name', '') in ['web_researcher', 'cortex_researcher']]\n",
        "\n",
        "def select_plan_text(output):\n",
        "    for m in output.get('messages', []):\n",
        "        if getattr(m, 'name', '') in ['initial_plan', 'replan']: return m.content\n",
        "    return \"\"\n",
        "\n",
        "def select_user_query(output):\n",
        "    return output.get(\"user_query\", \"\")\n",
        "\n",
        "def select_final_answer(output):\n",
        "    return output.get(\"final_answer\", \"\")\n",
        "\n",
        "def select_all(data):\n",
        "    return data\n",
        "\n",
        "# Custom State class with specific keys\n",
        "class State(MessagesState):\n",
        "    enabled_agents: Optional[List[str]]\n",
        "    # Current plan only: mapping from step number (as string) to step definition\n",
        "    plan: Optional[Dict[str, Dict[str, Any]]]\n",
        "    user_query: Optional[str]\n",
        "    current_step: int\n",
        "    replan_flag: Optional[bool]\n",
        "    last_reason: Optional[str]\n",
        "    # Replan attempts tracked per step number\n",
        "    replan_attempts: Optional[Dict[int, int]]\n",
        "    agent_query: Optional[str]\n",
        "    remaining_steps: RemainingSteps\n",
        "\n",
        "MAX_REPLANS = 2\n",
        "\n",
        "# # Create a Snowflake session\n",
        "# snowflake_connection_parameters = {\n",
        "#     \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "#     \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "#     \"password\": os.getenv(\"SNOWFLAKE_PAT\"),\n",
        "#     \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
        "#     \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
        "#     \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "#     \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
        "# }\n",
        "\n",
        "# snowpark_session = Session.builder.configs(\n",
        "#     snowflake_connection_parameters\n",
        "# ).create()\n",
        "\n",
        "# create a python repl tool for importing in the lessons\n",
        "repl = PythonREPL()\n",
        "\n",
        "@tool\n",
        "def python_repl_tool(\n",
        "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
        "):\n",
        "    \"\"\"Use this to execute python code. You will be used to execute python code\n",
        "    that generates charts. Only print the chart once.\n",
        "    This is visible to the user.\"\"\"\n",
        "    try:\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        return f\"Failed to execute. Error: {repr(e)}\"\n",
        "    result_str = (\n",
        "        f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
        "    )\n",
        "    return (\n",
        "        result_str\n",
        "        + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
        "    )\n",
        "\n",
        "reasoning_llm = ChatOpenAI(\n",
        "    model=os.environ[\"MODEL_REASONING\"],\n",
        "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n",
        ")\n",
        "\n",
        "@instrument(attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        \"retrieved_plan\": json.dumps(ret.update.get(\"plan\", {})), # 1. On capture le Plan (Output)\n",
        "        \"retrieved_query\": args[0].get(\"user_query\") or args[0].get(\"messages\", [HumanMessage(content=\"\")])[0].content}) # 2. On capture la Query User (Input) depuis l'√©tat (args[0] = state) # On essaie de lire 'user_query', sinon on prend le premier message\n",
        "def planner_node(state: State) \\\n",
        "        -> \"Command[Literal['executor']]\":\n",
        "    \"\"\"\n",
        "    Runs the planning LLM and stores the resulting plan in state.\n",
        "    \"\"\"\n",
        "    # 1. Invoke LLM with the planner prompt\n",
        "    llm_reply = reasoning_llm.invoke([plan_prompt(state)])\n",
        "\n",
        "    # 2. Validate JSON\n",
        "    try:\n",
        "        content_str = llm_reply.content if isinstance(llm_reply.content, str) else str(llm_reply.content)\n",
        "        parsed_plan = json.loads(content_str)\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(f\"Planner returned invalid JSON:\\n{llm_reply.content}\")\n",
        "\n",
        "    # 3. Store as current plan only\n",
        "    replan         = state.get(\"replan_flag\", False)\n",
        "    updated_plan: Dict[str, Any] = parsed_plan\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"plan\":         updated_plan,\n",
        "            \"messages\":     [HumanMessage(\n",
        "                                content=llm_reply.content,\n",
        "                                name=\"replan\" if replan else \"initial_plan\"\n",
        "                             )],\n",
        "            \"user_query\":   state.get(\"user_query\",\n",
        "                                      state[\"messages\"][0].content),\n",
        "           \"current_step\": 1 if not replan else state[\"current_step\"],\n",
        "           # Preserve replan flag so executor runs planned agent once before reconsidering\n",
        "           \"replan_flag\":  state.get(\"replan_flag\", False),\n",
        "           \"last_reason\":  \"\",\n",
        "           \"enabled_agents\": state.get(\"enabled_agents\"),\n",
        "        },\n",
        "        goto=\"executor\",\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create executor\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "@instrument(attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        \"retrieved_execution\": ret.update.get(\"messages\", [HumanMessage(content=\"\")])[-1].content}) # On capture la r√©ponse de l'executor depuis l'objet Command\n",
        "def executor_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"synthesizer\", \"planner\"]]:\n",
        "\n",
        "    plan: Dict[str, Any] = state.get(\"plan\", {})\n",
        "    step: int = state.get(\"current_step\", 1)\n",
        "\n",
        "    # 0) If we *just* replanned, run the planned agent once before reconsidering.\n",
        "    if state.get(\"replan_flag\"):\n",
        "        planned_agent = plan.get(str(step), {}).get(\"agent\")\n",
        "        return Command(\n",
        "            update={\n",
        "                \"replan_flag\": False,\n",
        "                \"current_step\": step + 1,  # advance because we executed the planned agent\n",
        "            },\n",
        "            goto=planned_agent,\n",
        "        )\n",
        "\n",
        "    # 1) Build prompt & call LLM\n",
        "    llm_reply = reasoning_llm.invoke([executor_prompt(state)])\n",
        "    try:\n",
        "        content_str = llm_reply.content if isinstance(llm_reply.content, str) else str(llm_reply.content)\n",
        "        parsed = json.loads(content_str)\n",
        "        replan: bool = parsed[\"replan\"]\n",
        "        goto: str   = parsed[\"goto\"]\n",
        "        reason: str = parsed[\"reason\"]\n",
        "        query: str  = parsed[\"query\"]\n",
        "    except Exception as exc:\n",
        "        raise ValueError(f\"Invalid executor JSON:\\n{llm_reply.content}\") from exc\n",
        "\n",
        "    # Upodate the state\n",
        "    updates: Dict[str, Any] = {\n",
        "        \"messages\": [HumanMessage(content=llm_reply.content, name=\"executor\")],\n",
        "        \"last_reason\": reason,\n",
        "        \"agent_query\": query,\n",
        "    }\n",
        "\n",
        "    # Replan accounting\n",
        "    replans: Dict[int, int] = state.get(\"replan_attempts\", {}) or {}\n",
        "    step_replans = replans.get(step, 0)\n",
        "\n",
        "    # 2) Replan decision\n",
        "    if replan:\n",
        "        if step_replans < MAX_REPLANS:\n",
        "            replans[step] = step_replans + 1\n",
        "            updates.update({\n",
        "                \"replan_attempts\": replans,\n",
        "                \"replan_flag\": True,     # ensure next turn executes the planned agent once\n",
        "                \"current_step\": step,    # stay on same step for the new plan\n",
        "            })\n",
        "            return Command(update=updates, goto=\"planner\")\n",
        "        else:\n",
        "            # Cap hit: skip this step; let next step (or synthesizer) handle termination\n",
        "            next_agent = plan.get(str(step + 1), {}).get(\"agent\", \"synthesizer\")\n",
        "            updates[\"current_step\"] = step + 1\n",
        "            return Command(update=updates, goto=next_agent)\n",
        "\n",
        "    # 3) Happy path: run chosen agent; advance only if following the plan\n",
        "    planned_agent = plan.get(str(step), {}).get(\"agent\")\n",
        "    updates[\"current_step\"] = step + 1 if goto == planned_agent else step\n",
        "    updates[\"replan_flag\"] = False\n",
        "    return Command(update=updates, goto=goto)\n",
        "\n",
        "# Set semantic model file (for analyst) and search service name\n",
        "# SEMANTIC_MODEL_FILE = \"@sales_intelligence.data.models/sales_metrics_model.yaml\"\n",
        "# CORTEX_SEARCH_SERVICE = \"sales_intelligence.data.sales_conversation_search\"\n",
        "\n",
        "# ---- Agent Setup ----\n",
        "# class CortexAgentArgs(BaseModel):\n",
        "#     query: str\n",
        "\n",
        "# class CortexAgentTool:\n",
        "    # ....\n",
        "\n",
        "    # def __init__(self, session: Session):\n",
        "    # ....\n",
        "\n",
        "    # def _consume_stream(self, stream):\n",
        "    # ....\n",
        "\n",
        "    # def run(self, query: str, **kwargs):\n",
        "    # ....\n",
        "\n",
        "# cortex_agent_tool = CortexAgentTool(session=snowpark_session)\n",
        "\n",
        "# NEW ------------------------------------\n",
        "# 1. Wikipedia Tool (Replaces Cortex Search - Unstructured)\n",
        "@tool\n",
        "def wikipedia_rag_tool(query: str):\n",
        "    \"\"\"\n",
        "    Retrieves unstructured information from Wikipedia to answer general knowledge questions.\n",
        "    Use this for definitions, history, summaries, or non-tabular data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        search_results = wikipedia.search(query, results=1)\n",
        "        if not search_results:\n",
        "            return \"No relevant Wikipedia pages found.\"\n",
        "\n",
        "        page = wikipedia.page(search_results[0], auto_suggest=False)\n",
        "        summary = page.content[:2000]\n",
        "        return f\"Source: {page.title}\\nURL: {page.url}\\n\\nContent:\\n{summary}\"\n",
        "    except Exception as e:\n",
        "        return f\"Wikipedia Error: {e}\"\n",
        "\n",
        "# 2. Wikidata SPARQL Tool (Replaces Cortex Analyst - Structured)\n",
        "@tool\n",
        "def wikidata_sparql_tool(query: str):\n",
        "    \"\"\"\n",
        "    Retrieves structured data (lists, counts, dates, facts) from Wikidata.\n",
        "    The input must be a natural language question. The tool will generate and execute SPARQL.\n",
        "    Use this when you need tables, specific data points, or relationships.\n",
        "    \"\"\"\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    # Internal helper to translate Natural Language -> SPARQL\n",
        "    # We use a small inline LLM call for this translation\n",
        "    translator_llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"], temperature=0)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Translate this question into a valid SPARQL query for Wikidata.\n",
        "    Question: {query}\n",
        "\n",
        "    Return ONLY the SPARQL code inside ```sparql ... ``` blocks.\n",
        "    Ensure prefixes like wdt: and wd: are correct.\n",
        "    Limit results to 10 unless specified.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = translator_llm.invoke(prompt)\n",
        "        content = response.content\n",
        "\n",
        "        # Extract SPARQL code block\n",
        "        if \"```sparql\" in content:\n",
        "            query_code = content.split(\"```sparql\")[1].split(\"```\")[0].strip()\n",
        "        elif \"```\" in content:\n",
        "            query_code = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "        else:\n",
        "            query_code = content.strip()\n",
        "\n",
        "        # Execute\n",
        "        sparql.setQuery(query_code)\n",
        "        results = sparql.query().convert()\n",
        "\n",
        "        # Parse JSON results into a string table\n",
        "        bindings = results[\"results\"][\"bindings\"]\n",
        "        if not bindings:\n",
        "            return \"No results found in Wikidata.\"\n",
        "\n",
        "        output_lines = []\n",
        "        for item in bindings:\n",
        "            row = []\n",
        "            for key in item:\n",
        "                row.append(f\"{key}: {item[key]['value']}\")\n",
        "            output_lines.append(\", \".join(row))\n",
        "\n",
        "        return f\"SPARQL Query Executed:\\n{query_code}\\n\\nResults:\\n\" + \"\\n\".join(output_lines)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"SPARQL Error: {e}\"\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "#from prompts import agent_system_prompt\n",
        "\n",
        "llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"])\n",
        "\n",
        "_cortex_llm_with_tools = llm.bind_tools([wikipedia_rag_tool, wikidata_sparql_tool])\n",
        "# cortex_agent = create_react_agent(llm, tools=[cortex_agent_tool.run], prompt=agent_system_prompt(f\"\"\"\n",
        "#         You are the Researcher. You can answer questions\n",
        "#         using customer deal data along with meeting notes.\n",
        "#         Do not take any further action.\n",
        "#     \"\"\"))\n",
        "# cortex_agent = create_react_agent(\n",
        "#     llm,\n",
        "#     tools=[wikipedia_rag_tool, wikidata_sparql_tool],\n",
        "#     max_iterations=3,\n",
        "#     prompt=agent_system_prompt(f\"\"\"\n",
        "#         You are the Cortex Researcher replacement.\n",
        "#         You have two tools:\n",
        "#         1. `wikidata_sparql_tool`: For STRUCTURED questions (lists, stats, facts).\n",
        "#         2. `wikipedia_rag_tool`: For UNSTRUCTURED questions (summaries, history).\n",
        "\n",
        "#         Choose the right tool based on the user's request.\n",
        "#      \"\"\"))\n",
        "\n",
        "@instrument(\n",
        "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\") if args[0].get(\"agent_query\") else None,\n",
        "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
        "            ret.update[\"messages\"][-1].content\n",
        "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
        "    },\n",
        ")\n",
        "def cortex_agents_research_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[\"executor\"]]:\n",
        "    \"\"\"\n",
        "    Cortex researcher using simple tool-calling (NO ReAct loop = NO recursion issue).\n",
        "\n",
        "    Flow:\n",
        "    1. LLM decides which tool to call\n",
        "    2. Execute that tool once\n",
        "    3. Return result\n",
        "    \"\"\"\n",
        "    query = state.get(\"agent_query\", state.get(\"user_query\", \"\"))\n",
        "\n",
        "    # Prompt that guides tool selection\n",
        "    prompt = f\"\"\"You are a research assistant. Use one of your available tools to answer this query.\n",
        "\n",
        "Available tools:\n",
        "- wikipedia_rag_tool: For general knowledge, summaries, descriptions, history\n",
        "- wikidata_sparql_tool: For structured data like lists, rankings, statistics, counts\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Call the most appropriate tool to answer this query.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Single LLM call - it will decide which tool to use\n",
        "        response = _cortex_llm_with_tools.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "        # Check if LLM made tool calls\n",
        "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "            results = []\n",
        "            for tool_call in response.tool_calls:\n",
        "                tool_name = tool_call.get(\"name\", \"\")\n",
        "                tool_args = tool_call.get(\"args\", {})\n",
        "\n",
        "                # Get the query argument (tools expect 'query' parameter)\n",
        "                tool_query = tool_args.get(\"query\", query)\n",
        "\n",
        "                # Execute the tool\n",
        "                try:\n",
        "                    if tool_name == \"wikipedia_rag_tool\":\n",
        "                        result = wikipedia_rag_tool.invoke({\"query\": tool_query})\n",
        "                    elif tool_name == \"wikidata_sparql_tool\":\n",
        "                        result = wikidata_sparql_tool.invoke({\"query\": tool_query})\n",
        "                    else:\n",
        "                        result = f\"Unknown tool: {tool_name}\"\n",
        "                except Exception as tool_error:\n",
        "                    result = f\"Tool {tool_name} failed: {str(tool_error)}\"\n",
        "\n",
        "                results.append(f\"=== {tool_name} ===\\n{result}\")\n",
        "\n",
        "            final_content = \"\\n\\n\".join(results)\n",
        "        else:\n",
        "            # LLM didn't call a tool - use its direct response or fallback\n",
        "            final_content = response.content if response.content else f\"No tool was called. Query: {query}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        final_content = f\"Research failed: {str(e)}\"\n",
        "\n",
        "    new_message = HumanMessage(content=final_content, name=\"cortex_researcher\")\n",
        "\n",
        "    return Command(\n",
        "        update={\"messages\": [new_message]},\n",
        "        goto=\"executor\",\n",
        "    )\n",
        "\n",
        "# ## Create Web Search Agent\n",
        "\n",
        "tavily_tool = TavilySearch(max_results=5)\n",
        "\n",
        "llm = ChatOpenAI(model=os.environ[\"MODEL_EXECUTOR\"])\n",
        "\n",
        "# Research agent and node\n",
        "web_search_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[tavily_tool],\n",
        "    prompt=agent_system_prompt(f\"\"\"\n",
        "        You are the Researcher. You can ONLY perform research by using the provided search tool (tavily_tool).\n",
        "        When you have found the necessary information, end your output.\n",
        "        Do NOT attempt to take further actions.\n",
        "    \"\"\"),\n",
        ")\n",
        "\n",
        "@instrument(\n",
        "    span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "    attributes=lambda ret, exception, *args, **kwargs: {\n",
        "        SpanAttributes.RETRIEVAL.QUERY_TEXT: args[0].get(\"agent_query\", \"\"),\n",
        "        SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: [\n",
        "            ret.update[\"messages\"][-1].content\n",
        "        ] if hasattr(ret, \"update\") else \"No tool call\",\n",
        "    },\n",
        ")\n",
        "def web_research_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[\"executor\"]]:\n",
        "    agent_query = state.get(\"agent_query\")\n",
        "    result = web_search_agent.invoke({\"messages\":agent_query}, config={\"recursion_limit\": 5})\n",
        "    messages = [HumanMessage(content=agent_query)] if isinstance(agent_query, str) else agent_query\n",
        "    result = web_search_agent.invoke({\"messages\": messages})\n",
        "    goto = \"executor\"\n",
        "    # wrap in a human message, as not all providers allow\n",
        "    # AI message at the last position of the input messages list\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"web_researcher\"\n",
        "    )\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of research agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "# ## Create Charting Agent\n",
        "\n",
        "# Chart generator agent and node\n",
        "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\n",
        "chart_agent = create_react_agent(\n",
        "    llm,\n",
        "    [python_repl_tool],\n",
        "    prompt=agent_system_prompt(\n",
        "        \"You can only generate charts. You are working with a researcher colleague. Print the chart first. Then, save the chart to a file in the current working directory and provide the path to the chart_summarizer.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "def chart_node(state: State) -> Command[Literal[\"chart_summarizer\"]]:\n",
        "    result = chart_agent.invoke(state)\n",
        "    # wrap in a human message, as not all providers allow\n",
        "    # AI message at the last position of the input messages list\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
        "    )\n",
        "    goto=\"chart_summarizer\"\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of chart agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create Chart Summary Agent\n",
        "\n",
        "chart_summary_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[],  # Add image processing tools if available/needed.\n",
        "    prompt=agent_system_prompt(\n",
        "        \"You can only summarize the chart that was generated by the chart generator to answer the user's question. You are working with a researcher colleague and a chart generator colleague. \"\n",
        "        + \"Your task is to generate a standalone, concise summary for the provided chart image saved at a local PATH, where the PATH should be and only be provided by your chart generator colleague. The summary should be no more than 3 sentences and should not mention the chart itself.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "def chart_summary_node(\n",
        "    state: State,\n",
        ") -> Command[Literal[END]]:\n",
        "    result = chart_summary_agent.invoke(state)\n",
        "    print(f\"Chart summarizer answer: {result['messages'][-1].content}\")\n",
        "    # Ensure the summary message is attributed to chart_summarizer for downstream use\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content, name=\"chart_summarizer\"\n",
        "    )\n",
        "    # Send to the end node\n",
        "    goto = END\n",
        "    return Command(\n",
        "        update={\n",
        "            # share internal message history of chart agent with other agents\n",
        "            \"messages\": result[\"messages\"],\n",
        "            \"final_answer\": result[\"messages\"][-1].content,\n",
        "        },\n",
        "        goto=goto,\n",
        "    )\n",
        "\n",
        "\n",
        "# ## Create a Synthesizer Agent\n",
        "def synthesizer_node(state: State) -> Command[Literal[END]]:\n",
        "    \"\"\"\n",
        "    Creates a concise, human‚Äëreadable summary of the entire interaction,\n",
        "    **purely in prose**.\n",
        "\n",
        "    It ignores structured tables or chart IDs and instead rewrites the\n",
        "    relevant agent messages (research results, chart commentary, etc.)\n",
        "    into a short final answer.\n",
        "    \"\"\"\n",
        "    # Gather informative messages for final synthesis\n",
        "    relevant_msgs = []\n",
        "    for m in state.get(\"messages\", []):\n",
        "        if getattr(m, \"name\", None) in (\"web_researcher\", \"cortex_researcher\", \"chart_generator\", \"chart_summarizer\"):\n",
        "            # FIX: Robustly handle content types and TRUNCATE huge outputs to avoid Token Limit Errors\n",
        "            raw_content = m.content\n",
        "            if isinstance(raw_content, list):\n",
        "                # Handle multimodal content (list of dicts) by flattening to string\n",
        "                text_content = \" \".join([str(item) for item in raw_content])\n",
        "            else:\n",
        "                text_content = str(raw_content) if raw_content else \"\"\n",
        "\n",
        "            # Truncate to ~15k chars per message to be safe (keeps context manageable)\n",
        "            if len(text_content) > 15000:\n",
        "                text_content = text_content[:15000] + \"... [TRUNCATED DUE TO LENGTH]\"\n",
        "\n",
        "            relevant_msgs.append(text_content)\n",
        "\n",
        "    # Fallback for user query extraction\n",
        "    messages_list = state.get(\"messages\", [])\n",
        "    if messages_list and hasattr(messages_list[0], \"content\"):\n",
        "        default_query = messages_list[0].content\n",
        "    else:\n",
        "        default_query = \"\"\n",
        "\n",
        "    user_question = state.get(\"user_query\", default_query)\n",
        "\n",
        "    synthesis_instructions = (\n",
        "            \"You are the Synthesizer. Use the context below to directly answer the user's question. \" # UPDATED THIS LINE\n",
        "            \"Perform any lightweight calculations, comparisons, or inferences required. \" # ADDED THIS LINE\n",
        "            \"Do not invent facts not supported by the context. If data is missing, say what's missing and, if helpful, \" # UPDATED THIS LINE\n",
        "            \"offer a clearly labeled best-effort estimate with assumptions.\\n\\n\" # ADDED THIS LINE\n",
        "            \"Produce a concise response that fully answers the question, with the following guidance:\\n\" # UPDATED THIS LINE\n",
        "            \"- Start with the direct answer (one short paragraph or a tight bullet list).\\n\"\n",
        "            \"- Include key figures from any 'Results:' tables (e.g., totals, top items).\\n\"\n",
        "            \"- If any message contains citations, include them as a brief 'Citations: [...]' line.\\n\"\n",
        "            \"- Keep the output crisp; avoid meta commentary or tool instructions.\"\n",
        "        )\n",
        "\n",
        "    summary_prompt = [\n",
        "        HumanMessage(content=(\n",
        "            f\"User question: {user_question}\\n\\n\"\n",
        "            f\"{synthesis_instructions}\\n\\n\"\n",
        "            f\"Context:\\n\\n\" + \"\\n\\n---\\n\\n\".join(relevant_msgs)\n",
        "        ))\n",
        "    ]\n",
        "    llm_reply = llm.invoke(summary_prompt)\n",
        "\n",
        "    reply_content = llm_reply.content\n",
        "    if isinstance(reply_content, list):\n",
        "        reply_text = \"\".join([c if isinstance(c, str) else str(c) for c in reply_content])\n",
        "    else:\n",
        "        reply_text = str(reply_content)\n",
        "    answer = reply_text.strip()\n",
        "    print(f\"Synthesizer answer: {answer}\")\n",
        "\n",
        "    return Command(\n",
        "        update={\n",
        "            \"final_answer\": answer,\n",
        "            \"messages\": [HumanMessage(content=answer, name=\"synthesizer\")],\n",
        "        },\n",
        "        goto=END,           # hand off to the END node\n",
        "    )\n",
        "\n",
        "##############################\n",
        "# Eval RAG Triad Evaluations #\n",
        "##############################\n",
        "provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
        "\n",
        "# Groundedness: retrieved contexts (RETRIEVAL spans) vs final answer (main output)\n",
        "f_groundedness = (Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
        "    .on({\"source\": Selector(span_type=SpanAttributes.SpanType.RETRIEVAL, span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS, collect_list=True,)})\n",
        "    .on_output())   # maps \"statement\" to the app main output in OTEL mode\n",
        "\n",
        "# Question/answer relevance: main input vs main output\n",
        "f_answer_relevance = (Feedback(provider.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
        "    .on_input()    # maps \"prompt\" (or input) from app main input\n",
        "    .on_output())   # maps \"response\" (or output) from app main output\n",
        "\n",
        "# Context relevance: main input vs each retrieved context chunk\n",
        "f_context_relevance = (Feedback(provider.context_relevance_with_cot_reasons, name=\"Context Relevance\")\n",
        "    .on_input()\n",
        "    .on({\"context\": Selector(span_type=SpanAttributes.SpanType.RETRIEVAL, span_attribute=SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS, collect_list=False,)})\n",
        "    .aggregate(np.mean))\n",
        "\n",
        "######################\n",
        "# Eval Goal-Plan-Act #\n",
        "######################\n",
        "gpa_eval_provider = OpenAI(model_engine=os.environ[\"MODEL_EVAL\"])\n",
        "\n",
        "f_logical_consistency = (Feedback(gpa_eval_provider.logical_consistency_with_cot_reasons, name=\"Logical Consistency\")\n",
        "    .on({\"trace\": Selector(trace_level=True)}))\n",
        "\n",
        "f_execution_efficiency = (Feedback(gpa_eval_provider.execution_efficiency_with_cot_reasons, name=\"Execution Efficiency\")\n",
        "    .on({\"trace\": Selector(trace_level=True)}))\n",
        "\n",
        "f_plan_adherence = (Feedback(gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Adherence\")\n",
        "    .on({\n",
        "            \"prompt\": Selector(span_attribute=\"retrieved_plan\"),      # On lit l'√©tiquette du Planner\n",
        "            \"response\": Selector(span_attribute=\"retrieved_execution\") # On lit l'√©tiquette de l'Executor\n",
        "        }))\n",
        "\n",
        "f_plan_quality = (\n",
        "    Feedback(gpa_eval_provider.relevance_with_cot_reasons, name=\"Plan Quality\")\n",
        "    .on({\n",
        "            # CORRECTION : On utilise le Selector sur l'attribut qu'on vient de cr√©er\n",
        "            \"prompt\": Selector(span_attribute=\"retrieved_query\"),\n",
        "\n",
        "            # Le plan (inchang√©)\n",
        "            \"response\": Selector(span_attribute=\"retrieved_plan\")\n",
        "        }))\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def display_eval_reason(text, width=800):\n",
        "    # Strip any trailing \"Score: X\" from the end of the text\n",
        "    raw_text = str(text).rstrip()\n",
        "    cleaned_text = re.sub(r\"\\s*Score:\\s*-?\\d+(?:\\.\\d+)?\\s*$\", \"\", raw_text, flags=re.IGNORECASE)\n",
        "    # Convert newlines to HTML line breaks, then wrap\n",
        "    html_text = cleaned_text.replace('\\n', '<br><br>')\n",
        "    display(HTML(f'<div style=\"font-size: 15px; word-wrap: break-word; width: {width}px;\">{html_text}</div>'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2X76Ix3smig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "\n",
        "load_dotenv(override=True)\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQnIEtuSrSej"
      },
      "source": [
        "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
        "<p> üíª &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
        "\n",
        "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT0DTuaYrSek"
      },
      "source": [
        "## 6.1 Add inline evaluations (skipped, already set in helpers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCfJPCIyrSel"
      },
      "source": [
        "## 6.2 Update the planning prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAUWHUW8rSel"
      },
      "source": [
        "Add pre-conditions, post-conditions, and goals to each step in the agent's plan.\n",
        "\n",
        "Adding this explicit detail helps the executor understand the goal of each step, which improves tool calling and agent decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfxnL3yJrSem"
      },
      "outputs": [],
      "source": [
        "#import helper\n",
        "#import prompts\n",
        "#from langchain.schema import HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "RECURSION_LIMIT = 15\n",
        "\n",
        "original_plan_prompt_fn = plan_prompt\n",
        "\n",
        "def patched_plan_prompt(state):\n",
        "    # FIX: Call the saved original function, NOT the global 'plan_prompt'\n",
        "    base = original_plan_prompt_fn(state).content\n",
        "    insertion = '\"action\": \"string\",\\n            \"pre_conditions\": [\"string\", ...],\\n            \"post_conditions\": [\"string\", ...],\\n            \"goal\": \"string\",'\n",
        "    base = base.replace('\"action\": \"string\",', insertion)\n",
        "\n",
        "    current_step = state.get(\"current_step\", 1)\n",
        "    used = max(0, int(current_step) - 1)\n",
        "    remaining = max(0, RECURSION_LIMIT - used)\n",
        "    base += (f\"\\n\\n<budget> Actions Budget Used: {used}, Max Budget Remaining: {remaining}.  ## IMPORTANT: Make the best use of the available resources. </budget>\")\n",
        "\n",
        "    return HumanMessage(content=base)\n",
        "\n",
        "plan_prompt = patched_plan_prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSd6pZTWrSem"
      },
      "source": [
        "## 6.3 Build the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUqYw7ehrSem"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "#from helper import State, planner_node, executor_node, chart_node, chart_summary_node, synthesizer_node, web_research_node, cortex_agents_research_node\n",
        "\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"executor\", executor_node)\n",
        "workflow.add_node(\"web_researcher\", web_research_node)\n",
        "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
        "workflow.add_node(\"chart_generator\", chart_node)\n",
        "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
        "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
        "\n",
        "workflow.add_edge(START, \"planner\")\n",
        "\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Preconfigure recursion_limit once (avoid passing it on every invoke).\n",
        "try: graph = graph.with_config({\"recursion_limit\": RECURSION_LIMIT})\n",
        "except Exception: pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKWpOyHKrSem"
      },
      "source": [
        "## 6.4 Create a TruLens session for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me21sjYtrSem",
        "outputId": "8b1ed10c-7080-4509-fc0f-d67b54542645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶ë Initialized with db url sqlite:///default.sqlite .\n",
            "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
          ]
        }
      ],
      "source": [
        "from trulens.core.session import TruSession\n",
        "from trulens.core.database.connector.default import DefaultDBConnector\n",
        "\n",
        "# Initialize connector with SQLite database one folder back\n",
        "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
        "\n",
        "# Create TruSession with the custom connector\n",
        "session = TruSession(connector=connector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLu2tCuErSen"
      },
      "source": [
        "## 6.5 Register the new version of the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t2-vzKprSen"
      },
      "source": [
        "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
        "    <p>üö® &nbsp; In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5NKZo3wOiKd",
        "outputId": "6e77eef7-4996-446f-f1d0-0c1fa744b419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "instrumenting <class 'langgraph.graph.state.StateGraph'> for base <class 'langgraph.graph.state.StateGraph'>\n",
            "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.graph.state.CompiledStateGraph'>\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting stream_mode\n",
            "instrumenting <class 'langgraph.graph.state.CompiledStateGraph'> for base <class 'langgraph.pregel.main.Pregel'>\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting astream_events\n",
            "\tinstrumenting invoke\n",
            "\tinstrumenting ainvoke\n",
            "\tinstrumenting stream\n",
            "\tinstrumenting astream\n",
            "\tinstrumenting stream_mode\n"
          ]
        }
      ],
      "source": [
        "from trulens.apps.langgraph import TruGraph\n",
        "from trulens.core.schema.feedback import FeedbackMode\n",
        "\n",
        "#from helper import f_answer_relevance, f_context_relevance, f_groundedness, f_logical_consistency, f_execution_efficiency, f_plan_adherence, f_plan_quality\n",
        "\n",
        "selected_feedbacks = [f_answer_relevance, f_context_relevance, f_groundedness, f_logical_consistency, f_execution_efficiency, f_plan_adherence, f_plan_quality]\n",
        "\n",
        "tru_recorder = TruGraph(\n",
        "    graph,\n",
        "    app_name=\"Research Data Agent\",\n",
        "    app_version=\"L6: Inline evals + sub-goals in planning prompt\",\n",
        "    feedbacks=selected_feedbacks,\n",
        "    feedback_mode=FeedbackMode.WITH_APP_THREAD,\n",
        "    selector_check_warning=True\n",
        "    # selector_nocheck=True # selector_check_warning=False, # selector_nocheck=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr5SfyxVpyzI",
        "outputId": "a3fd0739-e1d2-41ce-ed94-f018b46f8cc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNI [opentelemetry.instrumentation.instrumentor] Attempting to instrument while already instrumented\n"
          ]
        }
      ],
      "source": [
        "#@title Faire un seul TracerProvider global + export Phoenix + instrumentation LangChain ---\n",
        "from opentelemetry import trace\n",
        "from opentelemetry.sdk.trace import TracerProvider as SDKTracerProvider\n",
        "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
        "\n",
        "tp = trace.get_tracer_provider()\n",
        "if not isinstance(tp, SDKTracerProvider):\n",
        "    print(\"‚ö†Ô∏è TracerProvider OTEL inattendu. Assurez-vous que TRULENS_OTEL_TRACING=1 et que TruGraph est initialis√© avant ce bloc.\")\n",
        "\n",
        "# Ajouter un exporter Phoenix (OTLP HTTP) AU provider global (au lieu de laisser Phoenix/TruLens se battre)\n",
        "_exporter = None\n",
        "try:\n",
        "    from phoenix.otel import HTTPSpanExporter  # type: ignore\n",
        "    _exporter = HTTPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
        "except Exception:\n",
        "    try:\n",
        "        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter  # type: ignore\n",
        "        _exporter = OTLPSpanExporter(endpoint=os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Phoenix exporter non initialis√©: {e}\")\n",
        "\n",
        "if _exporter is not None:\n",
        "    try:\n",
        "        tp.add_span_processor(BatchSpanProcessor(_exporter))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Impossible d'ajouter le span processor Phoenix: {e}\")\n",
        "\n",
        "# Instrumentation OpenInference (spans LLM/tools) branch√©e sur le provider global TruLens\n",
        "try:\n",
        "    LangChainInstrumentor().instrument(tracer_provider=tp)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Instrumentation LangChain d√©j√† active ou erreur: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3st18lorSen"
      },
      "source": [
        "## 6.6 Re-test the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7InGlbkorSen"
      },
      "source": [
        "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\">\n",
        "    <p>üö® &nbsp;<b>Run Results:</b> In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLTWHO_TROs8"
      },
      "source": [
        "**Query 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "hDP1BTjyOoug",
        "outputId": "abac5b2d-17e6-4ce2-9b3a-68a792fca1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What are the top 5 largest cities in France by population ? Chart the population value for each.\n",
            "\u001b[1m[tasks]\u001b[0m {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}\n",
            "\u001b[1m[debug]\u001b[0m {'step': 1, 'timestamp': '2026-01-08T07:33:04.885667+00:00', 'type': 'task', 'payload': {'id': '48d6764c-f377-62fb-5ba8-0b900ca7d53a', 'name': 'planner', 'input': {'messages': [HumanMessage(content='What are the top 5 largest cities in France by population ? Chart the population value for each.', additional_kwargs={}, response_metadata={}, id='8c01f243-f24e-44bc-8530-e171942df065')], 'enabled_agents': ['cortex_researcher', 'web_researcher', 'chart_generator', 'chart_summarizer', 'synthesizer'], 'user_query': 'What are the top 5 largest cities in France by population ? Chart the population value for each.', 'remaining_steps': 14}, 'triggers': ('branch:to:planner',)}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR [trulens.core.otel.instrument] Error setting attributes: 'NoneType' object has no attribute 'update'\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1580204302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;34m\"enabled_agents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cortex_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"web_researcher\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_generator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chart_summarizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"synthesizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             }\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tasks\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"debug\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36minstrumented_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# Handle sync generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0minstrumented_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m                         \u001b[0;31m# Each chunk typically contains node updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2644\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/apps/langgraph/tru_graph.py\u001b[0m in \u001b[0;36mfiltered_wrapper\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minstrumented_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0;31m# Apply the wrapper using wrapt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36msync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"is_not_generator\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;31m# Check that there are no more entries in the generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mfunc_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                     _finalize_span(\n\u001b[0m\u001b[1;32m    304\u001b[0m                         \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36m_finalize_span\u001b[0;34m(span, span_type, func_name, func, func_exception, attributes, instance, args, kwargs, ret, only_set_user_defined_attributes, span_end_callbacks)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_exception\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mattributes_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trulens/core/otel/instrument.py\u001b[0m in \u001b[0;36mconvert_to_generator\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;31m# Run function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0;34m\"is_generator\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2330382181.py\u001b[0m in \u001b[0;36mplanner_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# 1. Invoke LLM with the planner prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mllm_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreasoning_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplan_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# 2. Validate JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1226\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         if (\n\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                     raw_response = (\n\u001b[0;32m-> 1354\u001b[0;31m                         self.root_client.chat.completions.with_raw_response.parse(\n\u001b[0m\u001b[1;32m   1355\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01j6q74hnze5sb1qc9pg0xfvpk` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 5604, Requested 751. Please try again in 3.55s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "with tru_recorder as recording:\n",
        "    query = \"What are the top 5 largest cities in France by population ? Chart the population value for each.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "                \"messages\": [HumanMessage(content=query)],\n",
        "                \"user_query\": query,\n",
        "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
        "            }\n",
        "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAkVRuo_rSen"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"‚ùå No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCMGL6KerSen"
      },
      "source": [
        "**Query 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehcg__GcRGAV"
      },
      "outputs": [],
      "source": [
        "with tru_recorder as recording:\n",
        "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "                \"messages\": [HumanMessage(content=query)],\n",
        "                \"user_query\": query,\n",
        "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
        "            }\n",
        "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc3-VIqQrSeo"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"‚ùå No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6faL_OQVrSeo"
      },
      "source": [
        "**Query 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_idPk2cRYd3"
      },
      "outputs": [],
      "source": [
        "with tru_recorder as recording:\n",
        "    query = \"Identify the largest laboratories studying and developping LLM, then find major topics of those companies in 2026, and find news article about top topics.\"\n",
        "    print(f\"Query: {query}\")\n",
        "    state = {\n",
        "                \"messages\": [HumanMessage(content=query)],\n",
        "                \"user_query\": query,\n",
        "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \"chart_generator\", \"chart_summarizer\", \"synthesizer\"],\n",
        "            }\n",
        "    graph.invoke(state, print_mode=[\"tasks\",\"updates\",\"debug\"])\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M-xIWy0rSeo"
      },
      "outputs": [],
      "source": [
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    print(f\"Query: {records.iloc[-1]['input']}\\n\")\n",
        "    print(f\"Output: {records.iloc[-1]['output']}\\n\")\n",
        "else:\n",
        "    print(\"‚ùå No records found. Check for errors in the output above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNQla7zrSeo"
      },
      "source": [
        "## 6.7 Launch TruLens dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sydoRIBerSeo"
      },
      "source": [
        "By comparing to the previous version, we can validate the changes.\n",
        "\n",
        "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkdZ9D11w1E7"
      },
      "outputs": [],
      "source": [
        "# @title üöÄ Launch Dashboard (Force Port 8502)\n",
        "!pip install -q trulens-dashboard\n",
        "from google.colab import output\n",
        "from trulens.core import TruSession\n",
        "import time\n",
        "\n",
        "session = TruSession()\n",
        "\n",
        "# Stop any existing dashboards\n",
        "try:\n",
        "    from trulens.dashboard import stop_dashboard\n",
        "    stop_dashboard(force=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"‚è≥ Starting Dashboard on port 8502...\")\n",
        "session.start_dashboard(port=8502, force=True)\n",
        "time.sleep(5) # Give it time to spin up\n",
        "\n",
        "print(\"‚úÖ Dashboard ready.\")\n",
        "#output.serve_kernel_port_as_iframe(8502, height=1000)\n",
        "output.serve_kernel_port_as_window(8502)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfjuz6uMxHsL"
      },
      "outputs": [],
      "source": [
        "# @title Alternative Analysis üìä > View Leaderboard as DataFrame\n",
        "from trulens.core import TruSession\n",
        "import pandas as pd\n",
        "\n",
        "max_records = 10\n",
        "session = TruSession()\n",
        "\n",
        "# Get the leaderboard (aggregates metrics by App ID)\n",
        "print(\"üìä Leaderboard:\")\n",
        "display(session.get_leaderboard())\n",
        "\n",
        "# OPTIONAL: Get all raw records to debug specific failures\n",
        "print(f\"\\nüìù Last {max_records} Raw Records:\")\n",
        "records, feedback = session.get_records_and_feedback()\n",
        "if not records.empty:\n",
        "    # Show relevant columns only\n",
        "    cols = ['input', 'output', 'latency', 'total_cost'] + [c for c in records.columns if 'Groundedness' in c or 'Relevance' in c]\n",
        "    # Filter columns that actually exist\n",
        "    valid_cols = [c for c in cols if c in records.columns]\n",
        "    display(records[valid_cols].tail(max_records))\n",
        "else:\n",
        "    print(\"No records found yet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4QHDdZrSep"
      },
      "source": [
        "**What other improvements could be also done?**\n",
        "- In this course, we focused on evaluating the end-to-end agent behavior. We could have also tested the behavior of each specialized agent separately to optimize their prompt and design.\n",
        "- We could have added other metrics for inline-evaluations.\n",
        "- We could also updated the prompt of the executor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xqf_odiwc73"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwNMoBWeuKr_"
      },
      "source": [
        "# Ajout des modules RAG et *SQL*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbs1wgUQuQ-g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHYjF8tuuZkA"
      },
      "source": [
        "# Vers l'optimisation\n",
        "\n",
        "> TODO: v√©rifications en cours: https://chatgpt.com/c/69583b8d-16b4-8327-9cc0-3b5baff84b01\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS9dJ-dku1UE"
      },
      "outputs": [],
      "source": [
        "#@title Instalation de l'optimiseur g√©n√©ratif Trace avec un m√©canisme de log externe\n",
        "!pip install \"git+https://github.com/doxav/NewTrace.git@json-logs-and-traces-IO\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBxC-S3-vkVD"
      },
      "source": [
        "R√©alisez dans la cellule ci-dessous deux exemples d'optimisations avec Trace avec succ√®s depuis un exemple existant:\n",
        "- https://github.com/AgentOpt/OpenTrace/tree/main/examples\n",
        "- https://agentopt.github.io/OpenTrace/#code-examples (attention la doc a √©t√© g√©n√©r√©e par IA g√©n√©rative, il peut y avoir des incoh√©rences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-KFEvjYweCz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29eOwLnQw7Ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RucotjFCucbE"
      },
      "outputs": [],
      "source": [
        "# @title Cr√©ation de trace_optimize_runtime.py (Attention le code de cette section du TP va √©voluer)\n",
        "%%writefile trace_optimize_runtime.py\n",
        "\"\"\"\n",
        "trace_optimize_runtime.py\n",
        "\n",
        "Pont minimal et **non-intrusif** entre :\n",
        "\n",
        "- des ex√©cutions LangGraph instrument√©es par TruLens (au format *records JSON* TruLens et/ou spans OpenTelemetry),\n",
        "- des feedbacks TruLens (RAG triad + GPA, ou toute autre m√©trique),\n",
        "- et l'optimiseur de la lib Trace/OptoPrime (fichiers `JSON_OTEL_trace_optim_demo_*.py`).\n",
        "\n",
        "Objectif : permettre une boucle \"run ‚Üí trace ‚Üí feedback ‚Üí optimise ‚Üí patch\" **sans modifier**\n",
        "le code du graphe LangGraph (n≈ìuds/agents) d√©j√† existant.\n",
        "\n",
        "Principes cl√©s\n",
        "-------------\n",
        "1) **Pr√©server le graphe causal** : on ne \"aplatit\" pas la trace. Les param√®tres `param.*`\n",
        "   sont attach√©s aux spans qui repr√©sentent *r√©ellement* les √©tapes (planner/executor/‚Ä¶),\n",
        "   et une span `evaluator` est ajout√©e uniquement pour porter `eval.*`.\n",
        "2) **Compatibilit√© double** :\n",
        "   - si vous avez une trace OTEL/OTLP (ex: TruLens OTEL activ√©), on l'utilise directement ;\n",
        "   - sinon, on reconstruit une trace OTLP minimale depuis un *Record* TruLens (JSON standard).\n",
        "3) **Optimisation de code** (pas seulement du prompt tuning) :\n",
        "   on expose du code comme param√®tre trainable via `param.__code_<key>` et on applique\n",
        "   les patches via compilation + hotpatch (in-place si possible, ou remplacement symbolique).\n",
        "\n",
        "Cette impl√©mentation vise une approche g√©n√©rique :\n",
        "- pas de fonctions nomm√©es \"for_l6\" ;\n",
        "- tout est pilot√© par des *configurations* (matchers, specs, targets).\n",
        "\n",
        "Pr√©-requis au runtime\n",
        "---------------------\n",
        "- TruLens : utilis√© pour capturer les records et produire les feedbacks.\n",
        "- (Optionnel) OpenTelemetry : si TruLens exporte des spans OTEL, on peut les \"flusher\".\n",
        "- Trace/Opto (repo Trace/opto) : utilis√© pour `otlp_traces_to_trace_json`, `ingest_tgj`,\n",
        "  et l'optimiseur OptoPrimeV2.\n",
        "\n",
        "Remarque : ce fichier ne d√©pend pas de LangGraph ni TruLens √† l'import.\n",
        "Il se contente de manipuler des JSON/dicts, et d'appliquer des patches Python.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime as _dt\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import textwrap\n",
        "import time\n",
        "import types\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Iterable,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Mapping,\n",
        "    MutableMapping,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    Tuple,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Types simples\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "JSONDict = Dict[str, Any]\n",
        "SpanDict = Dict[str, Any]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Utilitaires JSON / texte\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def safe_json_dumps(obj: Any, *, max_len: int = 4000) -> str:\n",
        "    \"\"\"\n",
        "    S√©rialise `obj` en JSON de mani√®re robuste (fallback str), puis tronque.\n",
        "\n",
        "    Args:\n",
        "        obj: objet √† s√©rialiser.\n",
        "        max_len: longueur max en caract√®res (au-del√†, on tronque).\n",
        "\n",
        "    Returns:\n",
        "        str JSON (ou string fallback), tronqu√©e si n√©cessaire.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s = json.dumps(obj, ensure_ascii=False, default=str)\n",
        "    except Exception:\n",
        "        s = str(obj)\n",
        "    if max_len and len(s) > max_len:\n",
        "        return s[: max_len - 3] + \"...\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def normalize_whitespace(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalise l√©g√®rement un texte (espaces, lignes vides) pour stabiliser des diffs.\n",
        "\n",
        "    Args:\n",
        "        s: texte.\n",
        "\n",
        "    Returns:\n",
        "        texte normalis√©.\n",
        "    \"\"\"\n",
        "    s2 = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    # √©vite de d√©truire la mise en forme: on enl√®ve juste les trailing spaces\n",
        "    s2 = \"\\n\".join(line.rstrip() for line in s2.splitlines())\n",
        "    return s2.strip() + (\"\\n\" if s2.endswith(\"\\n\") else \"\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# OTLP helpers (structure JSON)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _otlp_attr_value(value: Any) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Encode une valeur Python en valeur OTLP JSON (stringValue/doubleValue/intValue/boolValue).\n",
        "\n",
        "    Note:\n",
        "        Pour rester simple et compatible, on privil√©gie stringValue.\n",
        "        Les nombres sont encod√©s en doubleValue si possibles.\n",
        "\n",
        "    Returns:\n",
        "        dict au format OTLP \"AnyValue\".\n",
        "    \"\"\"\n",
        "    if isinstance(value, bool):\n",
        "        return {\"boolValue\": bool(value)}\n",
        "    if isinstance(value, int) and not isinstance(value, bool):\n",
        "        # OTLP accepte intValue sous forme de cha√Æne ou int selon l'impl; on met int.\n",
        "        return {\"intValue\": int(value)}\n",
        "    if isinstance(value, float):\n",
        "        return {\"doubleValue\": float(value)}\n",
        "    # fallback string\n",
        "    return {\"stringValue\": str(value)}\n",
        "\n",
        "\n",
        "def _otlp_kv(key: str, value: Any) -> Dict[str, Any]:\n",
        "    \"\"\"Construit un attribut OTLP (key/value).\"\"\"\n",
        "    return {\"key\": key, \"value\": _otlp_attr_value(value)}\n",
        "\n",
        "\n",
        "def otlp_is_payload(obj: Any) -> bool:\n",
        "    \"\"\"\n",
        "    D√©tecte si `obj` ressemble √† un payload OTLP traces JSON.\n",
        "\n",
        "    Args:\n",
        "        obj: objet quelconque.\n",
        "\n",
        "    Returns:\n",
        "        True si la structure contient `resourceSpans`.\n",
        "    \"\"\"\n",
        "    return isinstance(obj, dict) and \"resourceSpans\" in obj\n",
        "\n",
        "\n",
        "def otlp_iter_spans(otlp: JSONDict) -> Iterator[SpanDict]:\n",
        "    \"\"\"\n",
        "    It√®re sur tous les spans d'un payload OTLP.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (dict).\n",
        "\n",
        "    Yields:\n",
        "        chaque span (dict) *mutable*.\n",
        "    \"\"\"\n",
        "    for rs in otlp.get(\"resourceSpans\", []) or []:\n",
        "        for ss in rs.get(\"scopeSpans\", []) or []:\n",
        "            for sp in ss.get(\"spans\", []) or []:\n",
        "                yield sp\n",
        "\n",
        "\n",
        "def otlp_span_attrs_to_dict(span: SpanDict) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convertit la liste `span[\"attributes\"]` en dict {key: python_value}.\n",
        "\n",
        "    Args:\n",
        "        span: dict OTLP span.\n",
        "\n",
        "    Returns:\n",
        "        dict (valeurs simplifi√©es).\n",
        "    \"\"\"\n",
        "    out: Dict[str, Any] = {}\n",
        "    for kv in span.get(\"attributes\", []) or []:\n",
        "        k = kv.get(\"key\")\n",
        "        v = kv.get(\"value\", {})\n",
        "        if not k:\n",
        "            continue\n",
        "        # choisir un champ OTLP\n",
        "        if \"stringValue\" in v:\n",
        "            out[k] = v[\"stringValue\"]\n",
        "        elif \"doubleValue\" in v:\n",
        "            out[k] = float(v[\"doubleValue\"])\n",
        "        elif \"intValue\" in v:\n",
        "            out[k] = int(v[\"intValue\"])\n",
        "        elif \"boolValue\" in v:\n",
        "            out[k] = bool(v[\"boolValue\"])\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def otlp_set_span_attribute(span: SpanDict, key: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute ou remplace un attribut OTLP sur un span.\n",
        "\n",
        "    Args:\n",
        "        span: span OTLP mutable.\n",
        "        key: cl√© d'attribut.\n",
        "        value: valeur (sera encod√©e).\n",
        "    \"\"\"\n",
        "    attrs = span.get(\"attributes\")\n",
        "    if attrs is None:\n",
        "        attrs = []\n",
        "        span[\"attributes\"] = attrs\n",
        "\n",
        "    # replace if exists\n",
        "    for kv in attrs:\n",
        "        if kv.get(\"key\") == key:\n",
        "            kv[\"value\"] = _otlp_attr_value(value)\n",
        "            return\n",
        "\n",
        "    attrs.append(_otlp_kv(key, value))\n",
        "\n",
        "\n",
        "def otlp_get_trace_id(otlp: JSONDict) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Renvoie un traceId (hex) du payload OTLP si pr√©sent.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "\n",
        "    Returns:\n",
        "        traceId (32 hex chars) ou None.\n",
        "    \"\"\"\n",
        "    for sp in otlp_iter_spans(otlp):\n",
        "        tid = sp.get(\"traceId\")\n",
        "        if tid:\n",
        "            return tid\n",
        "    return None\n",
        "\n",
        "\n",
        "def _new_trace_id_hex() -> str:\n",
        "    \"\"\"G√©n√®re un traceId OTLP (32 hex chars).\"\"\"\n",
        "    return uuid.uuid4().hex  # 32 hex\n",
        "\n",
        "\n",
        "def _new_span_id_hex() -> str:\n",
        "    \"\"\"G√©n√®re un spanId OTLP (16 hex chars).\"\"\"\n",
        "    return f\"{random.getrandbits(64):016x}\"\n",
        "\n",
        "\n",
        "def ensure_otlp_shell(\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Construit un \"shell\" OTLP vide compatible avec `otlp_traces_to_trace_json`.\n",
        "\n",
        "    Args:\n",
        "        service_name: nom de ressource OTEL.\n",
        "        scope_name: nom du scope.\n",
        "\n",
        "    Returns:\n",
        "        dict OTLP avec `resourceSpans/scopeSpans/spans`.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"resourceSpans\": [\n",
        "            {\n",
        "                \"resource\": {\n",
        "                    \"attributes\": [\n",
        "                        _otlp_kv(\"service.name\", service_name),\n",
        "                    ]\n",
        "                },\n",
        "                \"scopeSpans\": [\n",
        "                    {\n",
        "                        \"scope\": {\"name\": scope_name, \"version\": \"\"},\n",
        "                        \"spans\": [],\n",
        "                    }\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "def otlp_append_span(otlp: JSONDict, span: SpanDict) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute un span √† la premi√®re scopeSpan du payload.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "        span: span dict.\n",
        "    \"\"\"\n",
        "    rs_list = otlp.setdefault(\"resourceSpans\", [])\n",
        "    if not rs_list:\n",
        "        otlp.update(ensure_otlp_shell())\n",
        "        rs_list = otlp[\"resourceSpans\"]\n",
        "    rs0 = rs_list[0]\n",
        "    ss_list = rs0.setdefault(\"scopeSpans\", [])\n",
        "    if not ss_list:\n",
        "        ss_list.append({\"scope\": {\"name\": \"trace_opt\", \"version\": \"\"}, \"spans\": []})\n",
        "    ss0 = ss_list[0]\n",
        "    spans = ss0.setdefault(\"spans\", [])\n",
        "    spans.append(span)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Capture OTEL -> OTLP (optionnel)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def try_attach_inmemory_span_exporter() -> Tuple[Optional[Any], Optional[Any], str]:\n",
        "    \"\"\"\n",
        "    Tente d'attacher un `InMemorySpanExporter` au TracerProvider global OpenTelemetry.\n",
        "\n",
        "    Pourquoi:\n",
        "        TruLens peut exporter des spans OTEL (OpenTelemetry). Si on peut accrocher un\n",
        "        exporter en m√©moire, on peut r√©cup√©rer la trace OTLP **sans** modifier le graphe.\n",
        "\n",
        "    Returns:\n",
        "        (exporter, processor, status)\n",
        "\n",
        "        - exporter: instance InMemorySpanExporter ou None\n",
        "        - processor: SimpleSpanProcessor ou None\n",
        "        - status: message (ok / warning / error)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from opentelemetry import trace as otel_trace  # type: ignore\n",
        "        from opentelemetry.sdk.trace.export import InMemorySpanExporter, SimpleSpanProcessor  # type: ignore\n",
        "    except Exception as e:\n",
        "        return None, None, f\"OpenTelemetry SDK indisponible: {e}\"\n",
        "\n",
        "    provider = otel_trace.get_tracer_provider()\n",
        "    if not hasattr(provider, \"add_span_processor\"):\n",
        "        return None, None, \"TracerProvider global n'a pas add_span_processor (provider non-SDK ?)\"\n",
        "\n",
        "    try:\n",
        "        exporter = InMemorySpanExporter()\n",
        "        processor = SimpleSpanProcessor(exporter)\n",
        "        provider.add_span_processor(processor)  # type: ignore[attr-defined]\n",
        "        return exporter, processor, \"ok\"\n",
        "    except Exception as e:\n",
        "        return None, None, f\"Erreur lors de l'attachement de l'exporter: {e}\"\n",
        "\n",
        "\n",
        "def flush_inmemory_exporter_to_otlp(\n",
        "    exporter: Any,\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"inmemory\",\n",
        "    clear: bool = True,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit les spans collect√©s par `InMemorySpanExporter` en payload OTLP JSON.\n",
        "\n",
        "    Args:\n",
        "        exporter: instance InMemorySpanExporter.\n",
        "        service_name: resource.service.name.\n",
        "        scope_name: scopeSpans.scope.name.\n",
        "        clear: si True, vider l'exporter apr√®s lecture.\n",
        "\n",
        "    Returns:\n",
        "        OTLP payload dict.\n",
        "    \"\"\"\n",
        "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
        "\n",
        "    spans = list(getattr(exporter, \"get_finished_spans\")() or [])\n",
        "    if clear and hasattr(exporter, \"clear\"):\n",
        "        exporter.clear()\n",
        "\n",
        "    for sp in spans:\n",
        "        try:\n",
        "            ctx = sp.get_span_context()\n",
        "            trace_id = f\"{ctx.trace_id:032x}\"\n",
        "            span_id = f\"{ctx.span_id:016x}\"\n",
        "        except Exception:\n",
        "            # fallback (rare)\n",
        "            trace_id = _new_trace_id_hex()\n",
        "            span_id = _new_span_id_hex()\n",
        "\n",
        "        parent_span_id = \"\"\n",
        "        try:\n",
        "            parent = getattr(sp, \"parent\", None)\n",
        "            if parent is not None:\n",
        "                parent_span_id = f\"{parent.span_id:016x}\"\n",
        "        except Exception:\n",
        "            parent_span_id = \"\"\n",
        "\n",
        "        name = getattr(sp, \"name\", \"span\")\n",
        "        start_ns = int(getattr(sp, \"start_time\", time.time_ns()))\n",
        "        end_ns = int(getattr(sp, \"end_time\", start_ns + 1_000_000))\n",
        "\n",
        "        attrs_list: List[Dict[str, Any]] = []\n",
        "        attrs = getattr(sp, \"attributes\", {}) or {}\n",
        "        if isinstance(attrs, dict):\n",
        "            for k, v in attrs.items():\n",
        "                # Pour rester robuste, on encode en string (Trace/otel_adapter sait parser stringValue).\n",
        "                attrs_list.append(_otlp_kv(str(k), safe_json_dumps(v, max_len=8000)))\n",
        "\n",
        "        otlp_append_span(\n",
        "            otlp,\n",
        "            {\n",
        "                \"traceId\": trace_id,\n",
        "                \"spanId\": span_id,\n",
        "                \"parentSpanId\": parent_span_id,\n",
        "                \"name\": str(name),\n",
        "                \"kind\": \"INTERNAL\",\n",
        "                \"startTimeUnixNano\": start_ns,\n",
        "                \"endTimeUnixNano\": end_ns,\n",
        "                \"attributes\": attrs_list,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    return otlp\n",
        "\n",
        "\n",
        "# TruLens record JSON -> OTLP (fallback si pas de spans OTEL disponibles)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def trulens_is_record(obj: Any) -> bool:\n",
        "    \"\"\"\n",
        "    Heuristique: d√©tecte si `obj` ressemble √† un Record TruLens (JSON standard).\n",
        "\n",
        "    Un Record TruLens (voir doc) contient typiquement `record_id` et `calls`.\n",
        "\n",
        "    Args:\n",
        "        obj: objet.\n",
        "\n",
        "    Returns:\n",
        "        True si on d√©tecte des champs \"record_id\" ou \"calls\".\n",
        "    \"\"\"\n",
        "    return isinstance(obj, dict) and (\"calls\" in obj or \"record_id\" in obj or \"main_input\" in obj)\n",
        "\n",
        "\n",
        "def _parse_dt_to_ns(value: Any) -> Optional[int]:\n",
        "    \"\"\"\n",
        "    Tente de parser des timestamps TruLens (perf.start_time / perf.end_time) vers ns Unix.\n",
        "\n",
        "    Formats accept√©s (best-effort):\n",
        "      - int / float : suppos√© √™tre des secondes (float) ou ns (int tr√®s grand).\n",
        "      - str ISO 8601 : ex \"2025-01-02T12:34:56.123Z\"\n",
        "      - datetime.\n",
        "\n",
        "    Returns:\n",
        "        int nanosecondes, ou None si impossible.\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return None\n",
        "\n",
        "    if isinstance(value, int):\n",
        "        # Heuristique: si tr√®s grand, c'est d√©j√† du ns\n",
        "        if value > 10_000_000_000_000:  # > ~1970 + 4h en ns\n",
        "            return value\n",
        "        # sinon secondes\n",
        "        return int(value * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, float):\n",
        "        return int(value * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, _dt.datetime):\n",
        "        if value.tzinfo is None:\n",
        "            value = value.replace(tzinfo=_dt.timezone.utc)\n",
        "        return int(value.timestamp() * 1_000_000_000)\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        s = value.strip()\n",
        "        # Z -> +00:00\n",
        "        if s.endswith(\"Z\"):\n",
        "            s = s[:-1] + \"+00:00\"\n",
        "        try:\n",
        "            dt = _dt.datetime.fromisoformat(s)\n",
        "            if dt.tzinfo is None:\n",
        "                dt = dt.replace(tzinfo=_dt.timezone.utc)\n",
        "            return int(dt.timestamp() * 1_000_000_000)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def _trulens_call_name(call: JSONDict) -> str:\n",
        "    \"\"\"\n",
        "    Produit un nom de span \"lisible\" pour un call TruLens.\n",
        "\n",
        "    TruLens record appelle ces objets `RecordAppCall` avec un champ `stack` contenant\n",
        "    des √©l√©ments `RecordAppCallMethod` incluant `path` et `method`.\n",
        "\n",
        "    Strat√©gie:\n",
        "      - si on a un `path`, on prend son dernier segment (souvent proche du nom de n≈ìud)\n",
        "      - sinon, on prend `method.name`\n",
        "      - sinon fallback \"call\"\n",
        "\n",
        "    Returns:\n",
        "        str\n",
        "    \"\"\"\n",
        "    stack = call.get(\"stack\") or []\n",
        "    top = stack[-1] if isinstance(stack, list) and stack else {}\n",
        "    method = (top.get(\"method\") or {}) if isinstance(top, dict) else {}\n",
        "    path = top.get(\"path\") if isinstance(top, dict) else None\n",
        "\n",
        "    # path est souvent un Lens (liste de segments)\n",
        "    last_seg: Optional[str] = None\n",
        "    if isinstance(path, (list, tuple)) and path:\n",
        "        last = path[-1]\n",
        "        if isinstance(last, str):\n",
        "            last_seg = last\n",
        "        else:\n",
        "            last_seg = str(last)\n",
        "    elif isinstance(path, str) and path:\n",
        "        # ex: \"nodes/planner\"\n",
        "        parts = re.split(r\"[\\\\/]+\", path)\n",
        "        last_seg = parts[-1] if parts else path\n",
        "\n",
        "    mname = None\n",
        "    if isinstance(method, dict):\n",
        "        mname = method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\")\n",
        "\n",
        "    if last_seg and last_seg not in {\"__call__\", \"invoke\", \"run\"}:\n",
        "        return str(last_seg)\n",
        "    if mname:\n",
        "        return str(mname)\n",
        "    return \"call\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class _CallSpan:\n",
        "    \"\"\"Structure interne pour reconstruire une hi√©rarchie approximative.\"\"\"\n",
        "    call_idx: int\n",
        "    name: str\n",
        "    call_id: str\n",
        "    start_ns: Optional[int]\n",
        "    end_ns: Optional[int]\n",
        "    stack_sig: Tuple[str, ...]\n",
        "    args: Any = None\n",
        "    rets: Any = None\n",
        "    error: Optional[str] = None\n",
        "    parent_idx: Optional[int] = None\n",
        "    span_id: str = field(default_factory=_new_span_id_hex)\n",
        "\n",
        "\n",
        "def _call_stack_signature(call: JSONDict) -> Tuple[str, ...]:\n",
        "    \"\"\"\n",
        "    Construit une signature (tuple) √† partir de `call.stack` pour aider √† inf√©rer la hi√©rarchie.\n",
        "\n",
        "    Returns:\n",
        "        tuple de strings.\n",
        "    \"\"\"\n",
        "    sig: List[str] = []\n",
        "    stack = call.get(\"stack\") or []\n",
        "    if not isinstance(stack, list):\n",
        "        return tuple()\n",
        "\n",
        "    for frame in stack:\n",
        "        if not isinstance(frame, dict):\n",
        "            continue\n",
        "        path = frame.get(\"path\")\n",
        "        method = frame.get(\"method\") or {}\n",
        "        # path normalis√©\n",
        "        if isinstance(path, (list, tuple)):\n",
        "            p = \"/\".join(str(x) for x in path)\n",
        "        else:\n",
        "            p = str(path) if path is not None else \"\"\n",
        "        m = \"\"\n",
        "        if isinstance(method, dict):\n",
        "            m = str(method.get(\"name\") or method.get(\"method_name\") or method.get(\"function_name\") or \"\")\n",
        "        sig.append(f\"{p}::{m}\".strip(\":\"))\n",
        "    return tuple(sig)\n",
        "\n",
        "\n",
        "def trulens_record_to_otlp(\n",
        "    record: JSONDict,\n",
        "    *,\n",
        "    service_name: str = \"trulens\",\n",
        "    scope_name: str = \"trulens_record\",\n",
        "    trace_id: Optional[str] = None,\n",
        "    include_root_span: bool = True,\n",
        "    max_io_chars: int = 4000,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit un Record TruLens (JSON) en payload OTLP minimal.\n",
        "\n",
        "    Cette conversion est un *fallback* quand vous n'avez pas de spans OTEL disponibles.\n",
        "    Elle reconstruit une hi√©rarchie de spans √† partir des `perf` timestamps (si pr√©sents),\n",
        "    sinon √† partir de signatures de stack (heuristique).\n",
        "\n",
        "    Args:\n",
        "        record: dict JSON TruLens (record).\n",
        "        service_name: service.name OTEL.\n",
        "        scope_name: scope OTEL.\n",
        "        trace_id: si fourni, utilis√© comme traceId.\n",
        "        include_root_span: ajoute un span racine \"record\" (recommand√©).\n",
        "        max_io_chars: taille max pour input.value / output.value.\n",
        "\n",
        "    Returns:\n",
        "        OTLP payload dict.\n",
        "    \"\"\"\n",
        "    if not trulens_is_record(record):\n",
        "        raise ValueError(\"L'objet fourni ne ressemble pas √† un Record TruLens JSON.\")\n",
        "\n",
        "    trace_id = trace_id or _new_trace_id_hex()\n",
        "    otlp = ensure_otlp_shell(service_name=service_name, scope_name=scope_name)\n",
        "\n",
        "    calls = record.get(\"calls\") or []\n",
        "    if not isinstance(calls, list):\n",
        "        calls = []\n",
        "\n",
        "    spans: List[_CallSpan] = []\n",
        "    for idx, call in enumerate(calls):\n",
        "        if not isinstance(call, dict):\n",
        "            continue\n",
        "        call_id = str(call.get(\"call_id\") or call.get(\"callId\") or f\"call-{idx}\")\n",
        "        name = _trulens_call_name(call)\n",
        "        perf = call.get(\"perf\") or {}\n",
        "        start_ns = _parse_dt_to_ns(perf.get(\"start_time\") or perf.get(\"startTime\") or perf.get(\"start\"))\n",
        "        end_ns = _parse_dt_to_ns(perf.get(\"end_time\") or perf.get(\"endTime\") or perf.get(\"end\"))\n",
        "        stack_sig = _call_stack_signature(call)\n",
        "\n",
        "        spans.append(\n",
        "            _CallSpan(\n",
        "                call_idx=idx,\n",
        "                name=name,\n",
        "                call_id=call_id,\n",
        "                start_ns=start_ns,\n",
        "                end_ns=end_ns,\n",
        "                stack_sig=stack_sig,\n",
        "                args=call.get(\"args\"),\n",
        "                rets=call.get(\"rets\"),\n",
        "                error=call.get(\"error\"),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Heuristique de hi√©rarchie:\n",
        "    # 1) si des timestamps existent pour la majorit√©, on utilise l'inclusion d'intervalles\n",
        "    # 2) sinon, on utilise la relation \"stack prefix\" la plus r√©cente\n",
        "    have_times = sum(1 for s in spans if s.start_ns is not None and s.end_ns is not None)\n",
        "    use_interval = have_times >= max(2, int(0.6 * len(spans))) if spans else False\n",
        "\n",
        "    if use_interval:\n",
        "        # Ordre: start asc, end desc (pour bien g√©rer les enveloppes)\n",
        "        spans_sorted = sorted(\n",
        "            spans,\n",
        "            key=lambda s: (\n",
        "                s.start_ns if s.start_ns is not None else 0,\n",
        "                -(s.end_ns if s.end_ns is not None else 0),\n",
        "            ),\n",
        "        )\n",
        "        stack: List[_CallSpan] = []\n",
        "        for s in spans_sorted:\n",
        "            s_start = s.start_ns if s.start_ns is not None else 0\n",
        "            # pop les spans qui se terminent avant le start courant\n",
        "            while stack and (stack[-1].end_ns is not None) and s_start >= (stack[-1].end_ns or 0):\n",
        "                stack.pop()\n",
        "            if stack:\n",
        "                s.parent_idx = stack[-1].call_idx\n",
        "            stack.append(s)\n",
        "        # spans_sorted contient des objets de la m√™me liste => parent_idx est appliqu√©\n",
        "    else:\n",
        "        # stack prefix: on mappe signature -> dernier idx\n",
        "        last_by_sig: Dict[Tuple[str, ...], int] = {}\n",
        "        for s in spans:\n",
        "            parent_sig = s.stack_sig[:-1] if s.stack_sig else tuple()\n",
        "            if parent_sig in last_by_sig:\n",
        "                s.parent_idx = last_by_sig[parent_sig]\n",
        "            # enregistrer ce call comme dernier pour sa signature\n",
        "            last_by_sig[s.stack_sig] = s.call_idx\n",
        "\n",
        "    # Root span optionnel\n",
        "    root_span_id = _new_span_id_hex()\n",
        "    root_end = max((s.end_ns or 0) for s in spans) if spans else time.time_ns()\n",
        "    root_start = min((s.start_ns or root_end) for s in spans) if spans else root_end - 1_000_000\n",
        "\n",
        "    if include_root_span:\n",
        "        root_span: SpanDict = {\n",
        "            \"traceId\": trace_id,\n",
        "            \"spanId\": root_span_id,\n",
        "            \"parentSpanId\": \"\",\n",
        "            \"name\": \"record\",\n",
        "            \"kind\": \"INTERNAL\",\n",
        "            \"startTimeUnixNano\": int(root_start),\n",
        "            \"endTimeUnixNano\": int(root_end),\n",
        "            \"attributes\": [\n",
        "                _otlp_kv(\"trulens.record_id\", record.get(\"record_id\") or record.get(\"recordId\") or \"\"),\n",
        "                _otlp_kv(\"input.value\", safe_json_dumps(record.get(\"main_input\"), max_len=max_io_chars)),\n",
        "                _otlp_kv(\"output.value\", safe_json_dumps(record.get(\"main_output\"), max_len=max_io_chars)),\n",
        "            ],\n",
        "        }\n",
        "        otlp_append_span(otlp, root_span)\n",
        "\n",
        "    # Convert calls to spans\n",
        "    now_ns = time.time_ns()\n",
        "    for s in spans:\n",
        "        start = s.start_ns or (now_ns + s.call_idx * 1_000_000)\n",
        "        end = s.end_ns or (start + 500_000)\n",
        "\n",
        "        parent_span_id = \"\"\n",
        "        if s.parent_idx is not None:\n",
        "            # retrouver le parent span_id\n",
        "            parent = next((p for p in spans if p.call_idx == s.parent_idx), None)\n",
        "            if parent is not None:\n",
        "                parent_span_id = parent.span_id\n",
        "        elif include_root_span:\n",
        "            parent_span_id = root_span_id\n",
        "\n",
        "        span: SpanDict = {\n",
        "            \"traceId\": trace_id,\n",
        "            \"spanId\": s.span_id,\n",
        "            \"parentSpanId\": parent_span_id,\n",
        "            \"name\": s.name,\n",
        "            \"kind\": \"INTERNAL\",\n",
        "            \"startTimeUnixNano\": int(start),\n",
        "            \"endTimeUnixNano\": int(end),\n",
        "            \"attributes\": [\n",
        "                _otlp_kv(\"trulens.call_id\", s.call_id),\n",
        "                _otlp_kv(\"input.value\", safe_json_dumps(s.args, max_len=max_io_chars)),\n",
        "                _otlp_kv(\"output.value\", safe_json_dumps(s.rets, max_len=max_io_chars)),\n",
        "            ],\n",
        "        }\n",
        "        if s.error:\n",
        "            span[\"attributes\"].append(_otlp_kv(\"error.value\", s.error))\n",
        "        otlp_append_span(otlp, span)\n",
        "\n",
        "    return otlp\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# S√©lection de spans / injection de param√®tres\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SpanMatcher:\n",
        "    \"\"\"\n",
        "    S√©lecteur de spans (OTLP) bas√© sur des heuristiques simples.\n",
        "\n",
        "    Vous pouvez matcher par :\n",
        "    - substring(s) sur le nom (`name_contains`)\n",
        "    - regex(s) sur le nom (`name_regex`)\n",
        "    - pr√©sence de certaines cl√©s d'attributs (`has_attrs`)\n",
        "    - substring(s) sur la valeur d'un attribut (`attr_contains`)\n",
        "\n",
        "    C'est volontairement simple pour rester g√©n√©rique et portable.\n",
        "    \"\"\"\n",
        "    name_contains: Tuple[str, ...] = ()\n",
        "    name_regex: Tuple[str, ...] = ()\n",
        "    has_attrs: Tuple[str, ...] = ()\n",
        "    attr_contains: Mapping[str, Tuple[str, ...]] = dataclasses.field(default_factory=dict)\n",
        "\n",
        "    def matches(self, span: SpanDict) -> bool:\n",
        "        \"\"\"Retourne True si `span` satisfait ce matcher.\"\"\"\n",
        "        name = str(span.get(\"name\") or \"\")\n",
        "        lname = name.lower()\n",
        "\n",
        "        if self.name_contains:\n",
        "            if not any(sub.lower() in lname for sub in self.name_contains):\n",
        "                return False\n",
        "\n",
        "        if self.name_regex:\n",
        "            ok = False\n",
        "            for pat in self.name_regex:\n",
        "                try:\n",
        "                    if re.search(pat, name):\n",
        "                        ok = True\n",
        "                        break\n",
        "                except re.error:\n",
        "                    continue\n",
        "            if not ok:\n",
        "                return False\n",
        "\n",
        "        if self.has_attrs or self.attr_contains:\n",
        "            attrs = otlp_span_attrs_to_dict(span)\n",
        "            if self.has_attrs:\n",
        "                if not all(k in attrs for k in self.has_attrs):\n",
        "                    return False\n",
        "            for k, subs in self.attr_contains.items():\n",
        "                v = str(attrs.get(k, \"\"))\n",
        "                lv = v.lower()\n",
        "                if not any(s.lower() in lv for s in subs):\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "def select_spans(otlp: JSONDict, matcher: SpanMatcher) -> List[SpanDict]:\n",
        "    \"\"\"\n",
        "    Retourne la liste des spans matching `matcher`.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP.\n",
        "        matcher: SpanMatcher.\n",
        "\n",
        "    Returns:\n",
        "        liste de spans (dicts mutables).\n",
        "    \"\"\"\n",
        "    return [sp for sp in otlp_iter_spans(otlp) if matcher.matches(sp)]\n",
        "\n",
        "\n",
        "def select_one_span(otlp: JSONDict, matcher: SpanMatcher) -> Optional[SpanDict]:\n",
        "    \"\"\"\n",
        "    Retourne le premier span matching (ou None).\n",
        "\n",
        "    Astuce:\n",
        "        Pratique pour choisir le parent d'un span evaluator, etc.\n",
        "\n",
        "    Returns:\n",
        "        span dict ou None.\n",
        "    \"\"\"\n",
        "    for sp in otlp_iter_spans(otlp):\n",
        "        if matcher.matches(sp):\n",
        "            return sp\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Sp√©cifications de param√®tres entra√Ænables\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ParamSpec:\n",
        "    \"\"\"\n",
        "    D√©crit un param√®tre √† :\n",
        "      1) exposer dans la trace (OTLP) via `param.<name>`\n",
        "      2) √©ventuellement appliquer au runtime lors d'une update.\n",
        "\n",
        "    Attributes:\n",
        "        name: nom logique (ex: \"planner_addendum\" ou \"__code_planner_node\").\n",
        "        get_value: fonction 0-arg retournant la valeur courante (str conseill√©).\n",
        "        apply_update: fonction (new_value:str) -> None, appliquant une update.\n",
        "        attach_to: SpanMatcher indiquant sur quel(s) span(s) √©crire l'attribut param.*.\n",
        "        trainable: si False, l'optimiseur ne doit pas toucher ce param.\n",
        "        description: description courte inject√©e c√¥t√© optimiseur (conseill√©e pour code).\n",
        "        normalize: optionnel, transforme la valeur avant injection (ex: normaliser espaces).\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    get_value: Callable[[], Any]\n",
        "    apply_update: Optional[Callable[[str], None]] = None\n",
        "    attach_to: Optional[SpanMatcher] = None\n",
        "    trainable: bool = True\n",
        "    description: str = \"\"\n",
        "    normalize: Optional[Callable[[str], str]] = normalize_whitespace\n",
        "\n",
        "    def value_as_str(self) -> str:\n",
        "        \"\"\"Renvoie la valeur courante en string (avec normalisation si configur√©e).\"\"\"\n",
        "        v = self.get_value()\n",
        "        s = v if isinstance(v, str) else safe_json_dumps(v, max_len=8000)\n",
        "        if self.normalize:\n",
        "            try:\n",
        "                s = self.normalize(s)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return s\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Prompt tuning g√©n√©rique (addendum non-intrusif)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class TextOverrideStore:\n",
        "    \"\"\"\n",
        "    Store simple (en m√©moire) pour des overrides textuels.\n",
        "\n",
        "    Usage typique:\n",
        "        store = TextOverrideStore()\n",
        "        store.set(\"planner_addendum\", \"...\")\n",
        "\n",
        "    On peut l'utiliser avec `wrap_prompt_builder_with_addendum` pour modifier\n",
        "    une fonction qui retourne un prompt (str ou BaseMessage LangChain).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._values: Dict[str, str] = {}\n",
        "\n",
        "    def get(self, key: str, default: str = \"\") -> str:\n",
        "        return str(self._values.get(key, default))\n",
        "\n",
        "    def set(self, key: str, value: str) -> None:\n",
        "        self._values[str(key)] = str(value)\n",
        "\n",
        "    def as_param_spec(\n",
        "        self,\n",
        "        *,\n",
        "        name: str,\n",
        "        attach_to: Optional[SpanMatcher],\n",
        "        trainable: bool = True,\n",
        "        description: str = \"\",\n",
        "    ) -> ParamSpec:\n",
        "        \"\"\"\n",
        "        Construit un ParamSpec \"texte\" connect√© √† ce store.\n",
        "\n",
        "        Args:\n",
        "            name: nom du param (cl√© dans le store).\n",
        "            attach_to: o√π accrocher dans la trace.\n",
        "            trainable: bool.\n",
        "            description: aide l'optimiseur.\n",
        "\n",
        "        Returns:\n",
        "            ParamSpec.\n",
        "        \"\"\"\n",
        "        return ParamSpec(\n",
        "            name=name,\n",
        "            get_value=lambda: self.get(name, \"\"),\n",
        "            apply_update=lambda v: self.set(name, v),\n",
        "            attach_to=attach_to,\n",
        "            trainable=trainable,\n",
        "            description=description,\n",
        "            normalize=normalize_whitespace,\n",
        "        )\n",
        "\n",
        "\n",
        "def _clone_langchain_message_with_content(msg: Any, new_content: str) -> Any:\n",
        "    \"\"\"\n",
        "    Clone un message LangChain (BaseMessage) en rempla√ßant `content`, best-effort.\n",
        "\n",
        "    On √©vite d'importer LangChain √† l'import du module; l'import est fait ici si possible.\n",
        "\n",
        "    Args:\n",
        "        msg: objet message.\n",
        "        new_content: contenu final.\n",
        "\n",
        "    Returns:\n",
        "        nouveau message (ou fallback str si impossible).\n",
        "    \"\"\"\n",
        "    # cas simple: string\n",
        "    if isinstance(msg, str):\n",
        "        return new_content\n",
        "\n",
        "    # Tentatives LangChain (pydantic/dataclass)\n",
        "    try:\n",
        "        from langchain_core.messages import BaseMessage  # type: ignore\n",
        "        if isinstance(msg, BaseMessage):\n",
        "            # Pydantic v2\n",
        "            if hasattr(msg, \"model_copy\"):\n",
        "                return msg.model_copy(update={\"content\": new_content})\n",
        "            # Pydantic v1\n",
        "            if hasattr(msg, \"copy\"):\n",
        "                try:\n",
        "                    return msg.copy(update={\"content\": new_content})\n",
        "                except TypeError:\n",
        "                    return msg.copy()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Generic: tenter reconstruction via __class__(**fields)\n",
        "    try:\n",
        "        if hasattr(msg, \"model_dump\"):\n",
        "            d = msg.model_dump()\n",
        "        elif hasattr(msg, \"dict\"):\n",
        "            d = msg.dict()\n",
        "        elif dataclasses.is_dataclass(msg):\n",
        "            d = dataclasses.asdict(msg)\n",
        "        else:\n",
        "            d = dict(getattr(msg, \"__dict__\", {}))\n",
        "        d[\"content\"] = new_content\n",
        "        cls = msg.__class__\n",
        "        return cls(**d)\n",
        "    except Exception:\n",
        "        # fallback string\n",
        "        return new_content\n",
        "\n",
        "\n",
        "def wrap_prompt_builder_with_addendum(\n",
        "    prompt_fn: Callable[..., Any],\n",
        "    *,\n",
        "    store: TextOverrideStore,\n",
        "    addendum_key: str,\n",
        "    header: str = \"\\n\\n# Addendum\\n\",\n",
        ") -> Callable[..., Any]:\n",
        "    \"\"\"\n",
        "    Wrap une fonction de prompt pour lui ajouter un \"addendum\" contr√¥l√© par `store`.\n",
        "\n",
        "    - Si `store.get(addendum_key)` est vide ‚Üí comportement inchang√©.\n",
        "    - Sinon ‚Üí on concat√®ne `original_content + header + addendum`.\n",
        "\n",
        "    Compatibilit√©:\n",
        "        - si la fonction renvoie un `str`, on renvoie un `str`\n",
        "        - si elle renvoie un message LangChain, on renvoie un message du m√™me type (best-effort)\n",
        "\n",
        "    Args:\n",
        "        prompt_fn: fonction originale (ex: prompts.plan_prompt).\n",
        "        store: TextOverrideStore.\n",
        "        addendum_key: nom du param d'override.\n",
        "        header: s√©parateur ajout√© avant l'addendum.\n",
        "\n",
        "    Returns:\n",
        "        fonction wrapper.\n",
        "    \"\"\"\n",
        "    def _wrapped(*args, **kwargs):\n",
        "        out = prompt_fn(*args, **kwargs)\n",
        "        add = store.get(addendum_key, \"\").strip()\n",
        "        if not add:\n",
        "            return out\n",
        "\n",
        "        # Extraire le contenu initial\n",
        "        if isinstance(out, str):\n",
        "            base = out\n",
        "        else:\n",
        "            base = str(getattr(out, \"content\", out))\n",
        "\n",
        "        new_content = base + header + add\n",
        "        return _clone_langchain_message_with_content(out, new_content)\n",
        "\n",
        "    # garder un minimum de metadata\n",
        "    try:\n",
        "        _wrapped.__name__ = getattr(prompt_fn, \"__name__\", \"prompt_wrapper\")\n",
        "        _wrapped.__doc__ = getattr(prompt_fn, \"__doc__\", None)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return _wrapped\n",
        "\n",
        "\n",
        "def inject_params_into_otlp(\n",
        "    otlp: JSONDict,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        "    *,\n",
        "    default_attach_to: Optional[SpanMatcher] = None,\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Ajoute des attributs `param.<name>` aux spans OTLP, selon les ParamSpec.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (sera copi√©).\n",
        "        param_specs: liste des ParamSpec √† exposer.\n",
        "        default_attach_to: matcher fallback si ParamSpec.attach_to est None.\n",
        "\n",
        "    Returns:\n",
        "        copie modifi√©e du payload OTLP.\n",
        "    \"\"\"\n",
        "    otlp2 = copy.deepcopy(otlp)\n",
        "    for spec in param_specs:\n",
        "        matcher = spec.attach_to or default_attach_to\n",
        "        if matcher is None:\n",
        "            # pas d'endroit o√π accrocher => skip\n",
        "            continue\n",
        "        val = spec.value_as_str()\n",
        "        for sp in select_spans(otlp2, matcher):\n",
        "            otlp_set_span_attribute(sp, f\"param.{spec.name}\", val)\n",
        "            otlp_set_span_attribute(sp, f\"param.{spec.name}.trainable\", \"true\" if spec.trainable else \"false\")\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "def add_evaluator_span(\n",
        "    otlp: JSONDict,\n",
        "    *,\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    parent_matcher: Optional[SpanMatcher] = None,\n",
        "    evaluator_span_name: str = \"evaluator\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Ajoute un span OTLP `evaluator` portant `eval.*` (score, m√©triques, raisons).\n",
        "\n",
        "    Important:\n",
        "        On n'attache PAS les `param.*` sur ce span (sauf si vous le d√©cidez),\n",
        "        pour √©viter l'optimisation bo√Æte noire.\n",
        "\n",
        "    Args:\n",
        "        otlp: payload OTLP (copi√©).\n",
        "        score: score global (0..1).\n",
        "        metrics: dict m√©triques (0..1).\n",
        "        reasons: texte explicatif.\n",
        "        parent_matcher: o√π accrocher l'evaluator (typiquement le span \"synthesizer\").\n",
        "        evaluator_span_name: nom de span.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP modifi√©.\n",
        "    \"\"\"\n",
        "    otlp2 = copy.deepcopy(otlp)\n",
        "    trace_id = otlp_get_trace_id(otlp2) or _new_trace_id_hex()\n",
        "\n",
        "    # choisir le parent span id\n",
        "    parent_span_id = \"\"\n",
        "    if parent_matcher is not None:\n",
        "        parent = select_one_span(otlp2, parent_matcher)\n",
        "        if parent is not None:\n",
        "            parent_span_id = str(parent.get(\"spanId\") or \"\")\n",
        "\n",
        "    # fallback: dernier span par endTimeUnixNano\n",
        "    if not parent_span_id:\n",
        "        spans = list(otlp_iter_spans(otlp2))\n",
        "        if spans:\n",
        "            spans_sorted = sorted(spans, key=lambda s: int(s.get(\"endTimeUnixNano\") or 0))\n",
        "            parent_span_id = str(spans_sorted[-1].get(\"spanId\") or \"\")\n",
        "\n",
        "    now_ns = time.time_ns()\n",
        "    span: SpanDict = {\n",
        "        \"traceId\": trace_id,\n",
        "        \"spanId\": _new_span_id_hex(),\n",
        "        \"parentSpanId\": parent_span_id,\n",
        "        \"name\": evaluator_span_name,\n",
        "        \"kind\": \"INTERNAL\",\n",
        "        \"startTimeUnixNano\": int(now_ns),\n",
        "        \"endTimeUnixNano\": int(now_ns + 500_000),\n",
        "        \"attributes\": [\n",
        "            _otlp_kv(\"eval.score\", str(float(score))),\n",
        "            _otlp_kv(\"eval.reasons\", reasons or \"\"),\n",
        "        ],\n",
        "    }\n",
        "    for k, v in metrics.items():\n",
        "        span[\"attributes\"].append(_otlp_kv(f\"eval.{k}\", str(float(v))))\n",
        "    # Optionnel: input/output.value pour donner de la \"mati√®re\" au graphe\n",
        "    span[\"attributes\"].append(_otlp_kv(\"input.value\", \"TruLens feedback\"))\n",
        "    span[\"attributes\"].append(_otlp_kv(\"output.value\", reasons or \"\"))\n",
        "\n",
        "    otlp_append_span(otlp2, span)\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "def coerce_to_otlp(\n",
        "    trace_or_record: Any,\n",
        "    *,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Convertit une entr√©e \"trace-like\" en OTLP.\n",
        "\n",
        "    Supporte:\n",
        "      - payload OTLP natif (dict avec `resourceSpans`)\n",
        "      - Record TruLens JSON (dict avec `calls` / `record_id`) -> OTLP minimal\n",
        "\n",
        "    Args:\n",
        "        trace_or_record: OTLP ou Record TruLens.\n",
        "        service_name: utilis√© si conversion TruLens -> OTLP.\n",
        "        scope_name: utilis√© si conversion TruLens -> OTLP.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP.\n",
        "    \"\"\"\n",
        "    if otlp_is_payload(trace_or_record):\n",
        "        return trace_or_record  # type: ignore[return-value]\n",
        "    if trulens_is_record(trace_or_record):\n",
        "        return trulens_record_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)  # type: ignore[arg-type]\n",
        "    raise ValueError(\"Entr√©e non reconnue: attendu OTLP ou Record TruLens JSON.\")\n",
        "\n",
        "\n",
        "def param_descriptions_from_specs(param_specs: Sequence[ParamSpec]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Construit un mapping semantic_name -> description √† partir des ParamSpec.\n",
        "\n",
        "    Astuce:\n",
        "        `semantic_name` correspond au nom de param tel qu'il appara√Æt dans Trace\n",
        "        (sans pr√©fixe runX:).\n",
        "\n",
        "    Args:\n",
        "        param_specs: specs.\n",
        "\n",
        "    Returns:\n",
        "        dict.\n",
        "    \"\"\"\n",
        "    out: Dict[str, str] = {}\n",
        "    for s in param_specs:\n",
        "        if s.description:\n",
        "            out[s.name] = s.description\n",
        "    return out\n",
        "\n",
        "\n",
        "def prepare_otlp_for_optimizer(\n",
        "    trace_or_record: Any,\n",
        "    *,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    default_param_attach_to: Optional[SpanMatcher] = None,\n",
        "    evaluator_parent_matcher: Optional[SpanMatcher] = None,\n",
        "    service_name: str = \"app\",\n",
        "    scope_name: str = \"trace_opt\",\n",
        ") -> JSONDict:\n",
        "    \"\"\"\n",
        "    Pipeline \"one-shot\" : (trace|record) -> OTLP -> inject params -> add evaluator.\n",
        "\n",
        "    Args:\n",
        "        trace_or_record: OTLP ou Record TruLens.\n",
        "        param_specs: param√®tres trainables √† exposer.\n",
        "        score: score global.\n",
        "        metrics: dict m√©triques.\n",
        "        reasons: texte explicatif.\n",
        "        default_param_attach_to: fallback pour ParamSpec.attach_to.\n",
        "        evaluator_parent_matcher: span parent pour l'evaluator.\n",
        "        service_name: service.name si conversion TruLens -> OTLP.\n",
        "        scope_name: scope.name si conversion TruLens -> OTLP.\n",
        "\n",
        "    Returns:\n",
        "        payload OTLP pr√™t √† √™tre ing√©r√© dans Trace.\n",
        "    \"\"\"\n",
        "    otlp0 = coerce_to_otlp(trace_or_record, service_name=service_name, scope_name=scope_name)\n",
        "    otlp1 = inject_params_into_otlp(otlp0, param_specs, default_attach_to=default_param_attach_to)\n",
        "    otlp2 = add_evaluator_span(\n",
        "        otlp1,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        reasons=reasons,\n",
        "        parent_matcher=evaluator_parent_matcher,\n",
        "    )\n",
        "    return otlp2\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Extraction m√©triques TruLens (depuis DataFrame row OU JSON)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def extract_metrics_from_mapping(\n",
        "    obj: Mapping[str, Any],\n",
        "    *,\n",
        "    metric_keys: Sequence[str],\n",
        "    default_metric: float = 0.5,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extrait des m√©triques depuis un mapping (dict-like) via des cl√©s.\n",
        "\n",
        "    Args:\n",
        "        obj: mapping (ex: row.to_dict()).\n",
        "        metric_keys: noms de colonnes / champs.\n",
        "        default_metric: fallback si manquant.\n",
        "\n",
        "    Returns:\n",
        "        dict m√©trique -> float.\n",
        "    \"\"\"\n",
        "    out: Dict[str, float] = {}\n",
        "    for k in metric_keys:\n",
        "        val = obj.get(k, default_metric)\n",
        "        try:\n",
        "            out[k] = float(val)\n",
        "        except Exception:\n",
        "            out[k] = float(default_metric)\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_score(\n",
        "    metrics: Mapping[str, float],\n",
        "    *,\n",
        "    weights: Optional[Mapping[str, float]] = None,\n",
        "    clamp_0_1: bool = True,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calcule un score scalaire √† partir d'un dict de m√©triques.\n",
        "\n",
        "    Args:\n",
        "        metrics: dict m√©trique -> float.\n",
        "        weights: dict m√©trique -> poids (sinon moyenne uniforme).\n",
        "        clamp_0_1: clamp le r√©sultat entre [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        float score.\n",
        "    \"\"\"\n",
        "    if not metrics:\n",
        "        return 0.5\n",
        "    if weights:\n",
        "        num = 0.0\n",
        "        den = 0.0\n",
        "        for k, v in metrics.items():\n",
        "            w = float(weights.get(k, 0.0))\n",
        "            num += w * float(v)\n",
        "            den += w\n",
        "        score = num / den if den > 0 else sum(float(v) for v in metrics.values()) / len(metrics)\n",
        "    else:\n",
        "        score = sum(float(v) for v in metrics.values()) / len(metrics)\n",
        "    if clamp_0_1:\n",
        "        score = max(0.0, min(1.0, score))\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "def select_latest_item(container: Any) -> Any:\n",
        "    \"\"\"\n",
        "    S√©lectionne \"le dernier √©l√©ment\" d'un container.\n",
        "\n",
        "    Supporte:\n",
        "      - pandas.DataFrame / pandas.Series via `.iloc[-1]`\n",
        "      - list/tuple via `[-1]`\n",
        "      - dict: renvoie tel quel (consid√©r√© d√©j√† comme 1 record)\n",
        "\n",
        "    Args:\n",
        "        container: objet.\n",
        "\n",
        "    Returns:\n",
        "        dernier √©l√©ment ou l'objet lui-m√™me (dict).\n",
        "\n",
        "    Raises:\n",
        "        ValueError si vide/incompatible.\n",
        "    \"\"\"\n",
        "    if container is None:\n",
        "        raise ValueError(\"container is None\")\n",
        "\n",
        "    if isinstance(container, dict):\n",
        "        return container\n",
        "\n",
        "    # pandas DataFrame/Series\n",
        "    if hasattr(container, \"iloc\"):\n",
        "        try:\n",
        "            if getattr(container, \"shape\", (0,))[0] == 0:\n",
        "                raise ValueError(\"container is empty\")\n",
        "            return container.iloc[-1]\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if isinstance(container, (list, tuple)):\n",
        "        if not container:\n",
        "            raise ValueError(\"container is empty\")\n",
        "        return container[-1]\n",
        "\n",
        "    raise ValueError(f\"Type non support√© pour select_latest_item: {type(container)}\")\n",
        "\n",
        "\n",
        "def extract_mapping(obj: Any) -> Mapping[str, Any]:\n",
        "    \"\"\"\n",
        "    Convertit best-effort un objet en mapping (dict-like).\n",
        "\n",
        "    Supporte:\n",
        "      - dict: renvoie tel quel\n",
        "      - pandas.Series: `.to_dict()`\n",
        "      - objets avec `model_dump()` (pydantic v2) ou `dict()` (pydantic v1)\n",
        "\n",
        "    Args:\n",
        "        obj: objet.\n",
        "\n",
        "    Returns:\n",
        "        mapping.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return obj\n",
        "\n",
        "    if hasattr(obj, \"to_dict\"):\n",
        "        try:\n",
        "            return obj.to_dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(obj, \"model_dump\"):\n",
        "        try:\n",
        "            return obj.model_dump()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(obj, \"dict\"):\n",
        "        try:\n",
        "            return obj.dict()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # fallback\n",
        "    return {\"value\": obj}\n",
        "\n",
        "\n",
        "def extract_trulens_record_json(obj: Any) -> Optional[JSONDict]:\n",
        "    \"\"\"\n",
        "    Extrait un Record TruLens JSON depuis diff√©rents conteneurs.\n",
        "\n",
        "    Cas g√©r√©s:\n",
        "      - si `obj` est d√©j√† un record dict (trulens_is_record) -> renvoie obj\n",
        "      - si `obj` est une row (Series/dict) contenant un champ `record_json` ou `record`\n",
        "        (dict ou JSON str) -> parse et renvoie.\n",
        "      - sinon None\n",
        "\n",
        "    Args:\n",
        "        obj: record-like.\n",
        "\n",
        "    Returns:\n",
        "        dict record ou None.\n",
        "    \"\"\"\n",
        "    if obj is None:\n",
        "        return None\n",
        "\n",
        "    if isinstance(obj, dict) and trulens_is_record(obj):\n",
        "        return obj\n",
        "\n",
        "    m = extract_mapping(obj)\n",
        "\n",
        "    for key in (\"record_json\", \"record\", \"record_jsons\", \"record_json_str\"):\n",
        "        if key in m:\n",
        "            raw = m.get(key)\n",
        "            if isinstance(raw, dict) and trulens_is_record(raw):\n",
        "                return raw\n",
        "            if isinstance(raw, str):\n",
        "                try:\n",
        "                    parsed = json.loads(raw)\n",
        "                    if isinstance(parsed, dict) and trulens_is_record(parsed):\n",
        "                        return parsed\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # parfois le record est stock√© sous une cl√© \"calls\" + \"record_id\" etc.\n",
        "    if isinstance(m, dict) and trulens_is_record(m):\n",
        "        return dict(m)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def render_feedback_text(\n",
        "    *,\n",
        "    score: float,\n",
        "    metrics: Mapping[str, float],\n",
        "    reasons: str = \"\",\n",
        "    extra: Optional[Mapping[str, Any]] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Rend un texte de feedback (√† passer √† OptoPrime) √† partir du score/m√©triques.\n",
        "\n",
        "    Args:\n",
        "        score: score global.\n",
        "        metrics: dict m√©triques.\n",
        "        reasons: texte explicatif (si dispo).\n",
        "        extra: infos additionnelles (ex: query, output, etc).\n",
        "\n",
        "    Returns:\n",
        "        str.\n",
        "    \"\"\"\n",
        "    parts = [f\"score={score:.3f}\"]\n",
        "    if metrics:\n",
        "        parts.append(\"metrics=\" + \", \".join(f\"{k}={v:.3f}\" for k, v in metrics.items()))\n",
        "    if reasons:\n",
        "        parts.append(\"reasons=\" + reasons.strip())\n",
        "    if extra:\n",
        "        for k, v in extra.items():\n",
        "            parts.append(f\"{k}={safe_json_dumps(v, max_len=600)}\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Code targets / patching (optimisation de code)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class CodeTarget:\n",
        "    \"\"\"\n",
        "    Cible de patching pour l'optimisation de code.\n",
        "\n",
        "    Un CodeTarget est associ√© √† un param√®tre trainable :\n",
        "        param.__code_<key>\n",
        "\n",
        "    Attributes:\n",
        "        key: identifiant stable (ex: \"planner_node\").\n",
        "        get_callable: fonction retournant l'objet callable courant √† patcher.\n",
        "        set_callable: optionnel, pour remplacer le symbole (module.attr = new_fn).\n",
        "        attach_to: SpanMatcher o√π accrocher le param√®tre code dans la trace.\n",
        "        trainable: bool.\n",
        "        description: aide l'optimiseur (ex: signature / r√¥le).\n",
        "    \"\"\"\n",
        "    key: str\n",
        "    get_callable: Callable[[], Callable[..., Any]]\n",
        "    set_callable: Optional[Callable[[Callable[..., Any]], None]] = None\n",
        "    attach_to: Optional[SpanMatcher] = None\n",
        "    trainable: bool = True\n",
        "    description: str = \"\"\n",
        "\n",
        "    @property\n",
        "    def param_name(self) -> str:\n",
        "        \"\"\"Nom du param√®tre expos√© dans la trace.\"\"\"\n",
        "        return f\"__code_{self.key}\"\n",
        "\n",
        "    def get_source(self) -> str:\n",
        "        \"\"\"\n",
        "        Extrait le code source de la fonction cible via inspect.getsource.\n",
        "\n",
        "        Returns:\n",
        "            str code python.\n",
        "        \"\"\"\n",
        "        fn = self.get_callable()\n",
        "        try:\n",
        "            return inspect.getsource(fn)\n",
        "        except OSError:\n",
        "            # ex: fonctions d√©finies dans un notebook sans source dispo\n",
        "            return f\"# Source indisponible pour {getattr(fn, '__name__', self.key)}\\n\"\n",
        "\n",
        "    def infer_description(self) -> str:\n",
        "        \"\"\"\n",
        "        D√©duit une description courte si `description` n'est pas fourni.\n",
        "\n",
        "        Returns:\n",
        "            str.\n",
        "        \"\"\"\n",
        "        if self.description:\n",
        "            return self.description\n",
        "        fn = self.get_callable()\n",
        "        try:\n",
        "            sig = str(inspect.signature(fn))\n",
        "        except Exception:\n",
        "            sig = \"(...)\"\n",
        "        return f\"{getattr(fn, '__name__', self.key)}{sig}\"\n",
        "\n",
        "\n",
        "def hotpatch_function_in_place(target_fn: Callable[..., Any], new_fn: Callable[..., Any]) -> None:\n",
        "    \"\"\"\n",
        "    Hotpatch \"in-place\" : remplace le bytecode (`__code__`) de `target_fn` par celui de `new_fn`.\n",
        "\n",
        "    Avantage:\n",
        "        Si LangGraph a captur√© une *r√©f√©rence* vers `target_fn`, le patch est effectif\n",
        "        sans recompiler le graphe.\n",
        "\n",
        "    Limites:\n",
        "        Ne marche pas si la fonction utilise des closures incompatibles.\n",
        "\n",
        "    Args:\n",
        "        target_fn: fonction originale (objet) utilis√©e par le graphe.\n",
        "        new_fn: fonction compil√©e √† partir d'un nouveau source.\n",
        "\n",
        "    Raises:\n",
        "        TypeError si pas patchable.\n",
        "    \"\"\"\n",
        "    if not (isinstance(target_fn, types.FunctionType) and isinstance(new_fn, types.FunctionType)):\n",
        "        raise TypeError(\"hotpatch_function_in_place ne supporte que des functions Python.\")\n",
        "    target_fn.__code__ = new_fn.__code__\n",
        "    target_fn.__defaults__ = new_fn.__defaults__\n",
        "    target_fn.__kwdefaults__ = new_fn.__kwdefaults__\n",
        "    target_fn.__annotations__ = getattr(new_fn, \"__annotations__\", {})\n",
        "    target_fn.__doc__ = getattr(new_fn, \"__doc__\", None)\n",
        "\n",
        "\n",
        "def compile_function_from_source(source: str, fn_name: str, *, glb: Optional[Dict[str, Any]] = None) -> Callable[..., Any]:\n",
        "    \"\"\"\n",
        "    Compile un source python contenant une d√©finition `def <fn_name>(...)` et renvoie cette fonction.\n",
        "\n",
        "    Args:\n",
        "        source: code python (doit d√©finir fn_name).\n",
        "        fn_name: nom de la fonction √† extraire.\n",
        "        glb: globals √† utiliser (permet d'acc√©der aux imports existants).\n",
        "\n",
        "    Returns:\n",
        "        function object.\n",
        "\n",
        "    Raises:\n",
        "        ValueError si la fonction n'existe pas apr√®s exec.\n",
        "    \"\"\"\n",
        "    glb = glb or {}\n",
        "    loc: Dict[str, Any] = {}\n",
        "    compiled = compile(source, \"<optimized>\", \"exec\")\n",
        "    exec(compiled, glb, loc)\n",
        "    fn = loc.get(fn_name) or glb.get(fn_name)\n",
        "    if not callable(fn):\n",
        "        raise ValueError(f\"Le source ne d√©finit pas la fonction attendue: {fn_name}\")\n",
        "    return fn  # type: ignore\n",
        "\n",
        "\n",
        "def apply_code_update(\n",
        "    *,\n",
        "    update_source: str,\n",
        "    target: CodeTarget,\n",
        "    patch_mode: str = \"in_place_or_replace\",\n",
        "    global_ns: Optional[Dict[str, Any]] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Applique un patch de code produit par l'optimiseur √† une cible.\n",
        "\n",
        "    Modes:\n",
        "      - \"in_place\": hotpatch sur l'objet callable actuel uniquement.\n",
        "      - \"replace\": remplace le symbole via target.set_callable (ou error si absent).\n",
        "      - \"in_place_or_replace\": tente in_place, sinon fallback replace.\n",
        "      - \"replace_and_in_place\": fait replace puis hotpatch (utile si le graphe a captur√© l'ancien objet).\n",
        "\n",
        "    Args:\n",
        "        update_source: code python complet (def ...).\n",
        "        target: CodeTarget.\n",
        "        patch_mode: strat√©gie.\n",
        "        global_ns: dict globals pour exec (souvent globals()).\n",
        "\n",
        "    Raises:\n",
        "        Exception si impossible.\n",
        "    \"\"\"\n",
        "    fn0 = target.get_callable()\n",
        "    fn_name = getattr(fn0, \"__name__\", None) or target.key\n",
        "    global_ns = global_ns or {}\n",
        "\n",
        "    new_fn = compile_function_from_source(update_source, fn_name, glb=global_ns)\n",
        "\n",
        "    if patch_mode == \"in_place\":\n",
        "        hotpatch_function_in_place(fn0, new_fn)\n",
        "        return\n",
        "\n",
        "    if patch_mode == \"replace\":\n",
        "        if target.set_callable is None:\n",
        "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
        "        target.set_callable(new_fn)\n",
        "        return\n",
        "\n",
        "    if patch_mode == \"replace_and_in_place\":\n",
        "        if target.set_callable is None:\n",
        "            raise ValueError(f\"target.set_callable manquant pour {target.key}\")\n",
        "        target.set_callable(new_fn)\n",
        "        # tenter hotpatch sur l'ancien objet\n",
        "        try:\n",
        "            hotpatch_function_in_place(fn0, new_fn)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return\n",
        "\n",
        "    # in_place_or_replace\n",
        "    try:\n",
        "        hotpatch_function_in_place(fn0, new_fn)\n",
        "        return\n",
        "    except Exception:\n",
        "        if target.set_callable is None:\n",
        "            raise\n",
        "        target.set_callable(new_fn)\n",
        "        return\n",
        "\n",
        "\n",
        "def build_code_param_specs(code_targets: Sequence[CodeTarget]) -> List[ParamSpec]:\n",
        "    \"\"\"\n",
        "    Convertit des CodeTarget en ParamSpec (pour injection OTLP + updates).\n",
        "\n",
        "    Args:\n",
        "        code_targets: cibles de code.\n",
        "\n",
        "    Returns:\n",
        "        liste ParamSpec.\n",
        "    \"\"\"\n",
        "    specs: List[ParamSpec] = []\n",
        "    for t in code_targets:\n",
        "        # closure pour get_source\n",
        "        def _make_getter(tt: CodeTarget) -> Callable[[], Any]:\n",
        "            return lambda: tt.get_source()\n",
        "\n",
        "        def _make_applier(tt: CodeTarget) -> Callable[[str], None]:\n",
        "            return lambda src: apply_code_update(update_source=src, target=tt, patch_mode=\"in_place_or_replace\", global_ns=globals())\n",
        "\n",
        "        specs.append(\n",
        "            ParamSpec(\n",
        "                name=t.param_name,\n",
        "                get_value=_make_getter(t),\n",
        "                apply_update=_make_applier(t),\n",
        "                attach_to=t.attach_to,\n",
        "                trainable=t.trainable,\n",
        "                description=t.infer_description(),\n",
        "                normalize=normalize_whitespace,\n",
        "            )\n",
        "        )\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Optimisation Trace/OptoPrime (√† partir d'OTLP)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# Petits wrappers autour des imports Trace pour rester optionnels\n",
        "def _require_trace_imports():\n",
        "    \"\"\"\n",
        "    Importe dynamiquement les composants Trace n√©cessaires.\n",
        "\n",
        "    Raises:\n",
        "        ImportError si la lib Trace/opto n'est pas install√©e/disponible.\n",
        "    \"\"\"\n",
        "    from opto.trace.io.otel_adapter import otlp_traces_to_trace_json  # type: ignore\n",
        "    from opto.trace.io.tgj_ingest import ingest_tgj  # type: ignore\n",
        "    from opto.trace.nodes import MessageNode, ParameterNode  # type: ignore\n",
        "    from opto.optimizers import OptoPrimeV2  # type: ignore\n",
        "    from opto.optimizers.utils import OptimizerPromptSymbolSetJSON  # type: ignore\n",
        "    from opto.trainer.algorithms.basic_algorithms import batchify  # type: ignore\n",
        "\n",
        "    return otlp_traces_to_trace_json, ingest_tgj, MessageNode, ParameterNode, OptoPrimeV2, OptimizerPromptSymbolSetJSON, batchify\n",
        "\n",
        "\n",
        "def find_target(nodes: Dict[str, Any], *, prefer_name_contains: str = \"evaluator\") -> Optional[Any]:\n",
        "    \"\"\"\n",
        "    Trouve le n≈ìud cible (MessageNode) √† optimiser.\n",
        "\n",
        "    Heuristique:\n",
        "      - si un MessageNode contient `prefer_name_contains` dans son nom ‚Üí on le prend\n",
        "      - sinon, on prend le \"dernier\" MessageNode rencontr√©.\n",
        "\n",
        "    Args:\n",
        "        nodes: dict name->node (r√©sultat ingest_tgj).\n",
        "        prefer_name_contains: substring.\n",
        "\n",
        "    Returns:\n",
        "        MessageNode ou None.\n",
        "    \"\"\"\n",
        "    _, _, MessageNode, _, _, _, _ = _require_trace_imports()\n",
        "    last = None\n",
        "    for n in nodes.values():\n",
        "        if isinstance(n, MessageNode):\n",
        "            last = n\n",
        "            if prefer_name_contains.lower() in (n.name or \"\").lower():\n",
        "                return n\n",
        "    return last\n",
        "\n",
        "\n",
        "def visualize_graph(nodes: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Visualise un graphe Trace (param√®tres + messages) sous forme texte.\n",
        "\n",
        "    Args:\n",
        "        nodes: dict name->node.\n",
        "\n",
        "    Returns:\n",
        "        str multi-ligne.\n",
        "    \"\"\"\n",
        "    _, _, MessageNode, ParameterNode, _, _, _ = _require_trace_imports()\n",
        "    params = []\n",
        "    messages = []\n",
        "    for name, node in nodes.items():\n",
        "        if isinstance(node, ParameterNode):\n",
        "            data = getattr(node, \"data\", \"\")\n",
        "            data_s = data[:80] + (\"...\" if isinstance(data, str) and len(data) > 80 else \"\")\n",
        "            params.append(f\"[PARAM] {node.name}: {data_s!r}\")\n",
        "        elif isinstance(node, MessageNode):\n",
        "            parents = getattr(node, \"parents\", []) or []\n",
        "            parent_names = [getattr(p, \"name\", \"?\") for p in parents]\n",
        "            messages.append(f\"[MSG] {node.name} ‚Üê {parent_names if parent_names else 'ROOT'}\")\n",
        "    return \"\\n\".join(params + messages)\n",
        "\n",
        "\n",
        "def check_reachability(target: Any, params: List[Any]) -> Dict[str, bool]:\n",
        "    \"\"\"\n",
        "    V√©rifie si chaque param√®tre est atteignable depuis `target` via les parents.\n",
        "\n",
        "    Utile pour d√©tecter un param√®tre accroch√© √† un span \"isol√©\" (non causalement reli√©).\n",
        "\n",
        "    Args:\n",
        "        target: MessageNode cible.\n",
        "        params: liste de ParameterNode.\n",
        "\n",
        "    Returns:\n",
        "        dict param.name -> bool.\n",
        "    \"\"\"\n",
        "    _, _, _, ParameterNode, _, _, _ = _require_trace_imports()\n",
        "    seen = set()\n",
        "    stack = [target]\n",
        "    reachable = set()\n",
        "    while stack:\n",
        "        node = stack.pop()\n",
        "        if node in seen:\n",
        "            continue\n",
        "        seen.add(node)\n",
        "        if hasattr(node, \"parents\"):\n",
        "            for p in getattr(node, \"parents\") or []:\n",
        "                if p not in seen:\n",
        "                    stack.append(p)\n",
        "        if isinstance(node, ParameterNode):\n",
        "            reachable.add(node.name)\n",
        "    return {p.name: p.name in reachable for p in params}\n",
        "\n",
        "\n",
        "def _remap_params_in_graph(node: Any, param_mapping: Dict[int, Any], visited=None) -> None:\n",
        "    \"\"\"\n",
        "    Remappe r√©cursivement des ParameterNode dans un graphe Trace.\n",
        "\n",
        "    Lorsqu'on r√©utilise un optimiseur entre it√©rations, on veut que les graphs\n",
        "    utilisent *les m√™mes objets ParameterNode* (ceux de l'optimiseur), sinon\n",
        "    l'optimiseur consid√®re des params diff√©rents.\n",
        "\n",
        "    Args:\n",
        "        node: n≈ìud courant.\n",
        "        param_mapping: dict id(old_param) -> optimizer_param.\n",
        "        visited: set d'ids d√©j√† visit√©s.\n",
        "    \"\"\"\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "\n",
        "    node_id = id(node)\n",
        "    if node_id in visited:\n",
        "        return\n",
        "    visited.add(node_id)\n",
        "\n",
        "    # Remap inputs\n",
        "    if hasattr(node, \"_inputs\") and isinstance(getattr(node, \"_inputs\"), dict):\n",
        "        inputs = getattr(node, \"_inputs\")\n",
        "        for key, input_node in list(inputs.items()):\n",
        "            in_id = id(input_node)\n",
        "            if in_id in param_mapping:\n",
        "                inputs[key] = param_mapping[in_id]\n",
        "            else:\n",
        "                _remap_params_in_graph(input_node, param_mapping, visited)\n",
        "\n",
        "    # Remap parents list\n",
        "    if hasattr(node, \"parents\") and isinstance(getattr(node, \"parents\"), list):\n",
        "        parents = getattr(node, \"parents\")\n",
        "        for i, parent in enumerate(list(parents)):\n",
        "            p_id = id(parent)\n",
        "            if p_id in param_mapping:\n",
        "                parents[i] = param_mapping[p_id]\n",
        "            else:\n",
        "                _remap_params_in_graph(parent, param_mapping, visited)\n",
        "\n",
        "\n",
        "def show_prompt_diff(before: str, after: str, *, context_lines: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Produit un diff textuel compact pour des prompts (ou code).\n",
        "\n",
        "    Args:\n",
        "        before: texte original.\n",
        "        after: texte modifi√©.\n",
        "        context_lines: lignes de contexte.\n",
        "\n",
        "    Returns:\n",
        "        diff str.\n",
        "    \"\"\"\n",
        "    import difflib\n",
        "    before_lines = normalize_whitespace(before).splitlines(True)\n",
        "    after_lines = normalize_whitespace(after).splitlines(True)\n",
        "    diff = difflib.unified_diff(before_lines, after_lines, fromfile=\"before\", tofile=\"after\", n=context_lines)\n",
        "    return \"\".join(diff)\n",
        "\n",
        "\n",
        "def compute_change_stats(before: str, after: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calcule des statistiques simples sur un changement (longueur, delta, etc).\n",
        "\n",
        "    Args:\n",
        "        before: texte original.\n",
        "        after: texte modifi√©.\n",
        "\n",
        "    Returns:\n",
        "        dict stats.\n",
        "    \"\"\"\n",
        "    b = before or \"\"\n",
        "    a = after or \"\"\n",
        "    return {\n",
        "        \"len_before\": len(b),\n",
        "        \"len_after\": len(a),\n",
        "        \"delta\": len(a) - len(b),\n",
        "        \"delta_pct\": ((len(a) - len(b)) / len(b) * 100.0) if len(b) else None,\n",
        "        \"lines_before\": b.count(\"\\n\") + 1 if b else 0,\n",
        "        \"lines_after\": a.count(\"\\n\") + 1 if a else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "def _ensure_param_descriptions_on_optimizer(optimizer: Any, params: Sequence[Any], desc_by_name: Mapping[str, str]) -> None:\n",
        "    \"\"\"\n",
        "    Ajoute/compl√®te les descriptions de param√®tres c√¥t√© optimiseur (si le champ existe).\n",
        "\n",
        "    Args:\n",
        "        optimizer: OptoPrimeV2.\n",
        "        params: ParameterNode (de l'optimiseur).\n",
        "        desc_by_name: mapping param_name -> description.\n",
        "    \"\"\"\n",
        "    # OptoPrime garde des params avec attributs name/data/desc (selon versions).\n",
        "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "        full_name = getattr(p, \"name\", \"\")\n",
        "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
        "        if semantic_name in desc_by_name:\n",
        "            if not getattr(p, \"desc\", \"\"):\n",
        "                try:\n",
        "                    p.desc = desc_by_name[semantic_name]\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RunResult:\n",
        "    \"\"\"\n",
        "    R√©sultat d'un run √† optimiser.\n",
        "\n",
        "    Attributes:\n",
        "        otlp: payload OTLP.\n",
        "        score: score global.\n",
        "        metrics: dict m√©triques.\n",
        "        feedback: texte de feedback (utilis√© par l'optimiseur).\n",
        "        meta: infos additionnelles (query, output, etc).\n",
        "    \"\"\"\n",
        "    otlp: JSONDict\n",
        "    score: float\n",
        "    metrics: Dict[str, float]\n",
        "    feedback: str\n",
        "    meta: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "\n",
        "def optimize_iteration(\n",
        "    runs: Sequence[RunResult],\n",
        "    *,\n",
        "    optimizer: Optional[Any],\n",
        "    llm_client: Any,\n",
        "    objective: str,\n",
        "    param_name_substrings: Sequence[str] = (\"__code_\",),\n",
        "    memory_size: int = 12,\n",
        "    verbose_graph: bool = False,\n",
        "    param_descriptions: Optional[Mapping[str, str]] = None,\n",
        "    prefer_target_name_contains: str = \"evaluator\",\n",
        ") -> Tuple[Dict[str, str], Any]:\n",
        "    \"\"\"\n",
        "    Ex√©cute une it√©ration OptoPrime sur un batch de runs.\n",
        "\n",
        "    Points importants (par rapport aux d√©mos) :\n",
        "      - compatible multi-runs (plusieurs requ√™tes) via batchify\n",
        "      - **remap** des ParameterNode quand on r√©utilise un optimiseur entre it√©rations,\n",
        "        afin que les nouveaux graphs pointent vers les *m√™mes objets* param√®tres\n",
        "      - filtre simple sur les param√®tres √† optimiser via `param_name_substrings`\n",
        "\n",
        "    Args:\n",
        "        runs: liste de runs (id√©alement plusieurs requ√™tes pour un signal plus robuste).\n",
        "        optimizer: OptoPrimeV2 existant (ou None au 1er tour).\n",
        "        llm_client: client LLM pour l'optimiseur (comme dans les d√©mos).\n",
        "        objective: instruction globale (\"maximize eval.score ...\").\n",
        "        param_name_substrings: filtre sur le champ `ParameterNode.name`.\n",
        "        memory_size: m√©moire de l'optimiseur.\n",
        "        verbose_graph: si True, imprime une visualisation texte des graphs.\n",
        "        param_descriptions: mapping semantic_name -> description (optionnel).\n",
        "        prefer_target_name_contains: substring pour choisir la cible (default \"evaluator\").\n",
        "\n",
        "    Returns:\n",
        "        (updates, optimizer)\n",
        "        updates: dict semantic_param_name -> new_value\n",
        "    \"\"\"\n",
        "    (\n",
        "        otlp_traces_to_trace_json,\n",
        "        ingest_tgj,\n",
        "        MessageNode,\n",
        "        ParameterNode,\n",
        "        OptoPrimeV2,\n",
        "        OptimizerPromptSymbolSetJSON,\n",
        "        batchify,\n",
        "    ) = _require_trace_imports()\n",
        "\n",
        "    # Mapping semantic_name -> optimizer ParameterNode (si optimizer d√©j√† cr√©√©)\n",
        "    opt_params_by_semantic: Dict[str, Any] = {}\n",
        "    if optimizer is not None:\n",
        "        for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "            full = getattr(p, \"name\", \"\") or \"\"\n",
        "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "            opt_params_by_semantic[semantic] = p\n",
        "\n",
        "    all_targets: List[Any] = []\n",
        "    all_feedbacks: List[str] = []\n",
        "    iter_params_by_semantic: Dict[str, Any] = {}\n",
        "\n",
        "    for i, run in enumerate(runs):\n",
        "        tgj_docs = list(\n",
        "            otlp_traces_to_trace_json(\n",
        "                run.otlp,\n",
        "                agent_id_hint=f\"run{i}\",\n",
        "                use_temporal_hierarchy=True,\n",
        "            )\n",
        "        )\n",
        "        if not tgj_docs:\n",
        "            continue\n",
        "        nodes = ingest_tgj(tgj_docs[0])\n",
        "\n",
        "        target = find_target(nodes, prefer_name_contains=prefer_target_name_contains)\n",
        "        if target is None:\n",
        "            continue\n",
        "\n",
        "        # Param√®tres trainables filtr√©s\n",
        "        params_in_graph: List[Any] = []\n",
        "        for n in nodes.values():\n",
        "            if isinstance(n, ParameterNode) and getattr(n, \"trainable\", False):\n",
        "                nname = getattr(n, \"name\", \"\") or \"\"\n",
        "                if any(sub in nname for sub in param_name_substrings):\n",
        "                    params_in_graph.append(n)\n",
        "\n",
        "        # Remap vers optimizer params si possible\n",
        "        id_mapping: Dict[int, Any] = {}\n",
        "        new_params_to_add: List[Any] = []\n",
        "        for p in params_in_graph:\n",
        "            full = getattr(p, \"name\", \"\") or \"\"\n",
        "            semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "            if semantic in opt_params_by_semantic:\n",
        "                id_mapping[id(p)] = opt_params_by_semantic[semantic]\n",
        "                iter_params_by_semantic.setdefault(semantic, opt_params_by_semantic[semantic])\n",
        "            else:\n",
        "                # nouveau param√®tre jamais vu\n",
        "                iter_params_by_semantic.setdefault(semantic, p)\n",
        "                new_params_to_add.append(p)\n",
        "\n",
        "        if id_mapping:\n",
        "            _remap_params_in_graph(target, id_mapping)\n",
        "\n",
        "        # si optimizer existe, on lui ajoute les nouveaux param√®tres\n",
        "        if optimizer is not None:\n",
        "            for p in new_params_to_add:\n",
        "                optimizer.parameters.append(p)  # type: ignore[attr-defined]\n",
        "                full = getattr(p, \"name\", \"\") or \"\"\n",
        "                semantic = full.split(\":\")[0].split(\"/\")[-1]\n",
        "                opt_params_by_semantic[semantic] = p\n",
        "\n",
        "        if verbose_graph:\n",
        "            print(\"\\n--- Graph (run\", i, \") ---\")\n",
        "            print(visualize_graph(nodes))\n",
        "\n",
        "        # Reachability diagnostic (apr√®s remap)\n",
        "        # On v√©rifie l'atteignabilit√© des param√®tres *utilis√©s* dans ce graph.\n",
        "        params_for_reach = list(iter_params_by_semantic.values())\n",
        "        reach = check_reachability(target, params_for_reach)\n",
        "        unreachable = [pname for pname, ok in reach.items() if not ok]\n",
        "        if unreachable:\n",
        "            print(f\"‚ö†Ô∏è Params non atteignables depuis target: {unreachable[:6]}{'...' if len(unreachable)>6 else ''}\")\n",
        "\n",
        "        all_targets.append(target)\n",
        "        all_feedbacks.append(run.feedback)\n",
        "\n",
        "    if not all_targets:\n",
        "        return {}, optimizer\n",
        "\n",
        "    # Cr√©er l'optimiseur au 1er tour\n",
        "    if optimizer is None:\n",
        "        optimizer = OptoPrimeV2(\n",
        "            iter_params_by_semantic.values(),\n",
        "            llm=llm_client,\n",
        "            memory_size=memory_size,\n",
        "            log=True,\n",
        "            optimizer_prompt_symbol_set=OptimizerPromptSymbolSetJSON(),\n",
        "            objective=objective,\n",
        "        )\n",
        "        # initialiser mapping pour la suite\n",
        "        opt_params_by_semantic = {\n",
        "            (p.name.split(\":\")[0].split(\"/\")[-1]): p for p in getattr(optimizer, \"parameters\", []) or []\n",
        "        }\n",
        "\n",
        "    # Ajouter des descriptions si fournies\n",
        "    if param_descriptions:\n",
        "        _ensure_param_descriptions_on_optimizer(optimizer, list(iter_params_by_semantic.values()), param_descriptions)\n",
        "\n",
        "    # Batchify et optimiser\n",
        "    batched_target = batchify(*all_targets).data\n",
        "    batched_feedback = batchify(*all_feedbacks).data\n",
        "\n",
        "    optimizer.zero_feedback()\n",
        "    optimizer.backward(batched_target, batched_feedback)\n",
        "    optimizer.step(verbose=False)\n",
        "\n",
        "    updates: Dict[str, str] = {}\n",
        "    for p in getattr(optimizer, \"parameters\", []) or []:\n",
        "        full_name = getattr(p, \"name\", \"\") or \"\"\n",
        "        semantic_name = full_name.split(\":\")[0].split(\"/\")[-1]\n",
        "        updates[semantic_name] = getattr(p, \"data\", \"\")\n",
        "\n",
        "    return updates, optimizer\n",
        "\n",
        "\n",
        "\n",
        "def apply_updates(\n",
        "    updates: Mapping[str, str],\n",
        "    *,\n",
        "    param_specs: Sequence[ParamSpec],\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Applique un dict d'updates (sortie OptoPrime) sur les ParamSpec.\n",
        "\n",
        "    Les ParamSpec dont `apply_update` est None sont ignor√©s.\n",
        "\n",
        "    Args:\n",
        "        updates: mapping semantic_name -> new_value.\n",
        "        param_specs: specs connus.\n",
        "\n",
        "    Returns:\n",
        "        dict \"appliqu√©\" : semantic_name -> \"ok\"/\"skipped\"/\"error:...\"\n",
        "    \"\"\"\n",
        "    specs_by_name = {s.name: s for s in param_specs}\n",
        "    out: Dict[str, str] = {}\n",
        "\n",
        "    for semantic, new_val in updates.items():\n",
        "        spec = specs_by_name.get(semantic)\n",
        "        if spec is None:\n",
        "            out[semantic] = \"skipped: unknown_param\"\n",
        "            continue\n",
        "        if spec.apply_update is None:\n",
        "            out[semantic] = \"skipped: no_apply_update\"\n",
        "            continue\n",
        "        try:\n",
        "            spec.apply_update(str(new_val))\n",
        "            out[semantic] = \"ok\"\n",
        "        except Exception as e:\n",
        "            out[semantic] = f\"error: {type(e).__name__}: {e}\"\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE8g-aJyxBOp"
      },
      "outputs": [],
      "source": [
        "# --- Trace/OptoPrime optimisation (non-intrusive) ---\n",
        "# Pr√©requis (d√©j√† faits dans le notebook) :\n",
        "#   - `graph` : LangGraph compil√©\n",
        "#   - `tru_recorder` : TruGraph (TruLens) qui wrap le graph\n",
        "#   - `session` : TruSession (ou adaptez ci-dessous)\n",
        "#   - `thread_config` : config LangGraph (ou adaptez)\n",
        "#\n",
        "# Et ajoutez le fichier `trace_optimize_runtime.py` √† c√¥t√© du notebook\n",
        "# (ou mettez-le dans votre PYTHONPATH).\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Assurez-vous de pouvoir importer le runtime\n",
        "if str(Path('.').resolve()) not in sys.path:\n",
        "    sys.path.append(str(Path('.').resolve()))\n",
        "\n",
        "import trace_optimize_runtime as tor\n",
        "\n",
        "# 2) (Optionnel) attacher un exporter OTEL en m√©moire (si TruLens OTEL est actif)\n",
        "exporter, processor, status = tor.try_attach_inmemory_span_exporter()\n",
        "print('OTEL in-memory exporter status:', status)\n",
        "\n",
        "# 3) Prompt addendums (tuning non intrusif) + wrappers\n",
        "store = tor.TextOverrideStore()\n",
        "\n",
        "# NOTE: dans L6, ces fonctions viennent souvent de `helper.py`.\n",
        "# Adaptez ces imports si besoin.\n",
        "\n",
        "plan_prompt = tor.wrap_prompt_builder_with_addendum(\n",
        "    plan_prompt, store=store, addendum_key='planner_addendum'\n",
        ")\n",
        "executor_prompt = tor.wrap_prompt_builder_with_addendum(\n",
        "    executor_prompt, store=store, addendum_key='executor_addendum'\n",
        ")\n",
        "\n",
        "planner_addendum = store.as_param_spec(\n",
        "    name='planner_addendum',\n",
        "    attach_to=tor.SpanMatcher(name_contains=('planner',)),\n",
        "    trainable=True,\n",
        "    description='Append-only instructions added to the planner prompt.'\n",
        ")\n",
        "executor_addendum = store.as_param_spec(\n",
        "    name='executor_addendum',\n",
        "    attach_to=tor.SpanMatcher(name_contains=('executor',)),\n",
        "    trainable=True,\n",
        "    description='Append-only instructions added to the executor prompt.'\n",
        ")\n",
        "\n",
        "# 4) Code targets (optimisation de code)\n",
        "# IMPORTANT: key doit √™tre stable; ici on utilise les noms de fonctions.\n",
        "# Si vos fonctions sont dans le notebook (pas de source inspectable), l'optimisation de code sera limit√©e.\n",
        "\n",
        "CODE_TARGETS = []\n",
        "try:\n",
        "    #from helper import planner_node, executor_node, synthesizer_node\n",
        "    CODE_TARGETS += [\n",
        "        tor.CodeTarget(\n",
        "            key='planner_node',\n",
        "            get_callable=lambda: planner_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=('planner',)),\n",
        "            description='LangGraph node that produces/updates the plan JSON.'\n",
        "        ),\n",
        "        tor.CodeTarget(\n",
        "            key='executor_node',\n",
        "            get_callable=lambda: executor_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=('executor',)),\n",
        "            description='LangGraph node that executes one plan step.'\n",
        "        ),\n",
        "        tor.CodeTarget(\n",
        "            key='synthesizer_node',\n",
        "            get_callable=lambda: synthesizer_node,\n",
        "            attach_to=tor.SpanMatcher(name_contains=('synthesizer',)),\n",
        "            description='Final synthesis / answer node.'\n",
        "        ),\n",
        "    ]\n",
        "except Exception as e:\n",
        "    print('Could not import code targets from helper:', e)\n",
        "\n",
        "code_param_specs = tor.build_code_param_specs(CODE_TARGETS)\n",
        "\n",
        "PARAM_SPECS = [planner_addendum, executor_addendum] + code_param_specs\n",
        "PARAM_DESC = tor.param_descriptions_from_specs(PARAM_SPECS)\n",
        "\n",
        "# 5) M√©triques TruLens (adapter si vos colonnes diff√®rent)\n",
        "METRIC_KEYS = [\n",
        "    'Groundedness',\n",
        "    'Answer Relevance',\n",
        "    'Context Relevance',\n",
        "    'Logical Consistency',\n",
        "    'Execution Efficiency',\n",
        "    'Plan Adherence',\n",
        "    'Plan Quality',\n",
        "]\n",
        "METRIC_WEIGHTS = {k: 1.0 for k in METRIC_KEYS}\n",
        "\n",
        "# 6) Objectif OptoPrime\n",
        "OBJECTIVE = \"\"\"You are optimizing a multi-agent LangGraph workflow.\n",
        "\n",
        "Goal:\n",
        "- Maximize eval.score (0..1), which aggregates eval.<metrics>.\n",
        "\n",
        "Constraints:\n",
        "- Keep function signatures unchanged.\n",
        "- Prefer minimal diffs.\n",
        "- Do not remove safety constraints.\n",
        "- If you edit code, keep it readable and deterministic.\n",
        "\"\"\"\n",
        "\n",
        "# 7) Petit helper pour ex√©cuter une requ√™te et collecter RunResult\n",
        "def run_query_collect(query: str):\n",
        "    # Clear exporter to isolate this run (OTEL path)\n",
        "    if exporter is not None and hasattr(exporter, 'clear'):\n",
        "        exporter.clear()\n",
        "\n",
        "    # Run LangGraph under TruLens recorder\n",
        "    with tru_recorder as recording:\n",
        "        out = graph.invoke({'messages': [('user', query)]}, config=thread_config)\n",
        "\n",
        "    # Get OTLP from OTEL exporter if available\n",
        "    otlp = None\n",
        "    if exporter is not None:\n",
        "        otlp = tor.flush_inmemory_exporter_to_otlp(\n",
        "            exporter,\n",
        "            service_name='l6',\n",
        "            scope_name='trulens_otel',\n",
        "            clear=True\n",
        "        )\n",
        "\n",
        "    # Fetch latest TruLens record + feedback\n",
        "    try:\n",
        "        recs, fbs = session.get_records_and_feedback(app_ids=[tru_recorder.app_id])\n",
        "    except Exception:\n",
        "        recs, fbs = session.get_records_and_feedback()\n",
        "\n",
        "    row = tor.select_latest_item(recs)\n",
        "    row_map = tor.extract_mapping(row)\n",
        "\n",
        "    # Fallback: if no OTEL spans, build OTLP from TruLens record JSON\n",
        "    if (otlp is None) or (len(list(tor.otlp_iter_spans(otlp))) == 0):\n",
        "        record_json = tor.extract_trulens_record_json(row)\n",
        "        if record_json is None:\n",
        "            raise RuntimeError('No OTEL spans and no record_json found. Cannot build trace.')\n",
        "        otlp = tor.trulens_record_to_otlp(record_json, service_name='l6', scope_name='trulens_record')\n",
        "\n",
        "    # Compute metrics + score\n",
        "    metrics = tor.extract_metrics_from_mapping(row_map, metric_keys=METRIC_KEYS, default_metric=0.5)\n",
        "    score = tor.compute_score(metrics, weights=METRIC_WEIGHTS)\n",
        "\n",
        "    # Best-effort reasons extraction\n",
        "    reasons = ''\n",
        "    for k in METRIC_KEYS:\n",
        "        for rk in (f'{k}_reasons', f'{k}.reasons', f'{k}_reason', f'{k}.reason'):\n",
        "            if rk in row_map and row_map[rk]:\n",
        "                reasons += f\"\\n[{k}] {row_map[rk]}\"\n",
        "\n",
        "    feedback = tor.render_feedback_text(\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        reasons=reasons,\n",
        "        extra={'query': query}\n",
        "    )\n",
        "\n",
        "    otlp_ready = tor.prepare_otlp_for_optimizer(\n",
        "        otlp,\n",
        "        param_specs=PARAM_SPECS,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        reasons=reasons,\n",
        "        evaluator_parent_matcher=tor.SpanMatcher(name_contains=('synthesizer',)),\n",
        "        service_name='l6',\n",
        "        scope_name='trace_opt',\n",
        "    )\n",
        "\n",
        "    return tor.RunResult(\n",
        "        otlp=otlp_ready,\n",
        "        score=score,\n",
        "        metrics=metrics,\n",
        "        feedback=feedback,\n",
        "        meta={'query': query}\n",
        "    )\n",
        "\n",
        "# 8) Boucle d'optimisation\n",
        "QUERIES = [\n",
        "    'Give me a plan and then answer: Compare France vs Germany GDP growth since 2010.',\n",
        "    'What are the key drivers of inflation in 2024-2025? Give citations.',\n",
        "]\n",
        "N_ITER = 2\n",
        "optimizer = None\n",
        "\n",
        "from opto.utils.llm import LLM\n",
        "LLM_CLIENT = LLM()\n",
        "\n",
        "for it in range(N_ITER):\n",
        "    runs = [run_query_collect(q) for q in QUERIES]\n",
        "    print(f'\\n=== Iteration {it} ===')\n",
        "    print('Scores:', [round(r.score, 3) for r in runs])\n",
        "\n",
        "    updates, optimizer = tor.optimize_iteration(\n",
        "        runs,\n",
        "        optimizer=optimizer,\n",
        "        llm_client=LLM_CLIENT,\n",
        "        objective=OBJECTIVE,\n",
        "        param_name_substrings=('__code_', 'planner_addendum', 'executor_addendum'),\n",
        "        memory_size=12,\n",
        "        verbose_graph=False,\n",
        "        param_descriptions=PARAM_DESC,\n",
        "        prefer_target_name_contains='evaluator',\n",
        "    )\n",
        "\n",
        "    applied = tor.apply_updates(updates, param_specs=PARAM_SPECS)\n",
        "    print('Applied:', {k:v for k,v in applied.items() if v != 'skipped: unknown_param'})\n",
        "\n",
        "print('\\nFinal addendums:')\n",
        "print('planner_addendum:\\n', store.get('planner_addendum'))\n",
        "print('executor_addendum:\\n', store.get('executor_addendum'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
